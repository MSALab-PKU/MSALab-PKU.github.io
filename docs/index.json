[{"authors":["chan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"56687b7fcbaceb3c4ed6d5b35f5c4e2a","permalink":"https://MSALab-PKU.github.io/author/chang-han/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chang-han/","section":"authors","summary":"","tags":null,"title":"Chang Han","type":"authors"},{"authors":["csun"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"be26fadc8643576e89752107a6fac3ee","permalink":"https://MSALab-PKU.github.io/author/chao-sun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chao-sun/","section":"authors","summary":"","tags":null,"title":"Chao Sun","type":"authors"},{"authors":["jgbai"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f3a76360065a3359cd90463bea76f6dc","permalink":"https://MSALab-PKU.github.io/author/jiangang-bai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiangang-bai/","section":"authors","summary":"","tags":null,"title":"Jiangang Bai","type":"authors"},{"authors":["jzwu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5a0c8c939b2c265a97fc98ef25b2275b","permalink":"https://MSALab-PKU.github.io/author/jianzong-wu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jianzong-wu/","section":"authors","summary":"","tags":null,"title":"Jianzong Wu","type":"authors"},{"authors":["jbhu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3ed410aac5b4f052f61a64ee927c14cf","permalink":"https://MSALab-PKU.github.io/author/jingbo-hu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jingbo-hu/","section":"authors","summary":"","tags":null,"title":"Jingbo Hu","type":"authors"},{"authors":["mlzhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"dfedae070636eb70976aabdd2adb501f","permalink":"https://MSALab-PKU.github.io/author/mingliang-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mingliang-zhang/","section":"authors","summary":"","tags":null,"title":"Mingliang Zhang","type":"authors"},{"authors":["slxu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"68ff3b5443a4ade8666c671627510859","permalink":"https://MSALab-PKU.github.io/author/shilin-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shilin-xu/","section":"authors","summary":"","tags":null,"title":"Shilin Xu","type":"authors"},{"authors":["YoungTimmy"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b21181240bd935c7972236f1a9d91a55","permalink":"https://MSALab-PKU.github.io/author/tianmeng-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tianmeng-yang/","section":"authors","summary":"","tags":null,"title":"Tianmeng Yang","type":"authors"},{"authors":["xtli"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b15b1dd349fae079cb30852b753352c5","permalink":"https://MSALab-PKU.github.io/author/xiangtai-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiangtai-li/","section":"authors","summary":"","tags":null,"title":"Xiangtai Li","type":"authors"},{"authors":["ymyang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a8a395d490050dea6e89bf73db842358","permalink":"https://MSALab-PKU.github.io/author/yaming-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yaming-yang/","section":"authors","summary":"","tags":null,"title":"Yaming Yang","type":"authors"},{"authors":["yichen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2c3dc4301bb02fdc04677974cb73959a","permalink":"https://MSALab-PKU.github.io/author/yiren-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yiren-chen/","section":"authors","summary":"","tags":null,"title":"Yiren Chen","type":"authors"},{"authors":["yjwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b675a23079a8b1fab71d06f02d9bf8fe","permalink":"https://MSALab-PKU.github.io/author/yujing-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yujing-wang/","section":"authors","summary":"","tags":null,"title":"Yujing Wang","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fe6f7929557162f2fd6242ea4c7d82ee","permalink":"https://MSALab-PKU.github.io/author/yunhai-tong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yunhai-tong/","section":"authors","summary":"","tags":null,"title":"Yunhai Tong","type":"authors"},{"authors":["zmiao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"db1f392f869907e89db7137c5bed49a2","permalink":"https://MSALab-PKU.github.io/author/zheng-miao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zheng-miao/","section":"authors","summary":"","tags":null,"title":"Zheng Miao","type":"authors"},{"authors":["zjlin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9e45d01cc28563997db9fc47cedc0a1e","permalink":"https://MSALab-PKU.github.io/author/zhengjie-lin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhengjie-lin/","section":"authors","summary":"","tags":null,"title":"Zhengjie Lin","type":"authors"},{"authors":["Jianzong Wu","Xiangtai Li","Yunhai Tong"],"categories":[],"content":"","date":1690070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690070400,"objectID":"70e1dc4f57d88168319a4e998ffa1a60","permalink":"https://MSALab-PKU.github.io/publication/cgg-2023/","publishdate":"2023-07-23T00:00:00Z","relpermalink":"/publication/cgg-2023/","section":"publication","summary":"In this work, we focus on open vocabulary instance segmentation to expand a segmentation model to classify and segment instance-level novel categories. Previous approaches have relied on massive caption datasets and complex pipelines to establish one-to-one mappings between image regions and words in captions. However, such methods build noisy supervision by matching non-visible words to image regions, such as adjectives and verbs. Meanwhile, context words are also important for inferring the existence of novel objects as they show high inter-correlations with novel categories. To overcome these limitations, we devise a joint **Caption Grounding and Generation (CGG)** framework, which incorporates a novel grounding loss that only focuses on matching object nouns to improve learning efficiency. We also introduce a caption generation head that enables additional supervision and contextual modeling as a complementation to the grounding loss. Our analysis and results demonstrate that grounding and generation components complement each other, significantly enhancing the segmentation performance for novel classes. Experiments on the COCO dataset with two settings: Open Vocabulary Instance Segmentation (OVIS) and Open Set Panoptic Segmentation (OSPS) demonstrate the superiority of the CGG. Specifically, CGG achieves a substantial improvement of **6.8% mAP** for novel classes without extra data on the OVIS task and **15% PQ** improvements for novel classes on the OSPS benchmark.","tags":[],"title":"Betrayed by captions: Joint caption grounding and generation for open vocabulary instance segmentation","type":"publication"},{"authors":["Xiangtai Li","Yunhai Tong"],"categories":[],"content":"","date":1642377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642377600,"objectID":"773ced82ad3461e5a60b3dcdb72ce5ee","permalink":"https://MSALab-PKU.github.io/publication/transvoc-v2-2021/","publishdate":"2022-01-17T00:00:00Z","relpermalink":"/publication/transvoc-v2-2021/","section":"publication","summary":"Detection Transformer (DETR) and Deformable DETR have been proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance as previous complex hand-crafted detectors. However, their performance on Video Object Detection (VOD) has *not* been well explored. In this paper, we present **TransVOD**, the first end-to-end video object detection system based on spatial-temporal Transformer architectures. The first goal of this paper is to streamline the pipeline of VOD, effectively removing the need for many hand-crafted components for feature aggregation, *e.g.*, optical flow model, relation networks. Besides, benefited from the object query design in DETR, our method does not need complicated post-processing methods such as Seq-NMS. In particular, we present a temporal Transformer to aggregate both the spatial object queries and the feature memories of each frame. Our temporal transformer consists of two components: Temporal Query Encoder (TQE) to fuse object queries, and Temporal Deformable Transformer Decoder (TDTD) to obtain current frame detection results. These designs boost the strong baseline deformable DETR by a significant margin (3 %-4 % mAP) on the ImageNet VID dataset. TransVOD yields comparable performances on the benchmark of ImageNet VID. Then, we present two improved versions of TransVOD including TransVOD++ and TransVOD Lite. The former fuses object-level information into object query via dynamic convolution while the latter models the entire video clips as the output to speed up the inference time. We give detailed analysis of all three models in the experiment part. In particular, our proposed TransVOD++ sets a new state-of-the-art record in terms of accuracy on ImageNet VID with 90.0 % mAP. Our proposed TransVOD Lite also achieves the best speed and accuracy trade-off with 83.7 % mAP while running at around 30 FPS on a single V100 GPU device. Code and models will be available for further research.","tags":[],"title":"TransVOD: End-to-end Video Object Detection with Spatial-Temporal Transformers","type":"publication"},{"authors":["Xiangtai Li","Yunhai Tong"],"categories":[],"content":"","date":1639440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639440000,"objectID":"d505565b6758fd247d528b58ed246576","permalink":"https://MSALab-PKU.github.io/publication/boundarysqueeze-2021/","publishdate":"2021-12-14T00:00:00Z","relpermalink":"/publication/boundarysqueeze-2021/","section":"publication","summary":"This paper proposes a novel method for high-quality image segmentation of both objects and scenes. Inspired by the dilation and erosion operations in morphological image processing techniques, the pixel-level image segmentation problems are treated as squeezing object boundaries. From this perspective, a novel and efficient **Boundary Squeeze** module is proposed. This module is used to squeeze the object boundary from both inner and outer directions, which contributes to precise mask representation. A bi-directionally flow-based warping process is proposed to generate such squeezed feature representation, and two specific loss signals are designed to supervise the squeezing process. The Boundary Squeeze module can be easily applied to both instance and semantic segmentation tasks as a plug-and-play module by building on top of some existing methods. Moreover, the proposed module is lightweighted, and thus has potential for practical usage. Experiment results show that our simple yet effective design can produce high-quality results on several different datasets. Besides, several other metrics on the boundary are used to prove the effectiveness of our method over previous work. Our approach yields significant improvement on challenging COCO and Cityscapes datasets for both instance and semantic segmentation, and outperforms previous state-of-the-art PointRend in both accuracy and speed under the same setting. Codes and models will be published at https://github.com/lxtGH/BSSeg.","tags":[],"title":"BoundarySqueeze: Image Segmentation as Boundary Squeezing","type":"publication"},{"authors":["Xiangtai Li","Yunhai Tong"],"categories":[],"content":"","date":1634688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634688000,"objectID":"1b2dd644c6e68a4ccf6397d2912d3a75","permalink":"https://MSALab-PKU.github.io/publication/transvoc-2021/","publishdate":"2021-10-20T00:00:00Z","relpermalink":"/publication/transvoc-2021/","section":"publication","summary":"Recently, DETR and Deformable DETR have been proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance as previous complex hand-crafted detectors. However, their performance on Video Object Detection (VOD) has not been well explored. In this paper, we present TransVOD, an end-to-end video object detection model based on a spatial-temporal Transformer architecture. The goal of this paper is to streamline the pipeline of VOD, effectively removing the need for many hand-crafted components for feature aggregation, *e.g.*, optical flow, recurrent neural networks, relation networks. Besides, benefited from the object query design in DETR, our method does not need complicated post-processing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean. In particular, we present temporal Transformer to aggregate both the spatial object queries and the feature memories of each frame. Our temporal Transformer consists of three components: Temporal Deformable Transformer Encoder (TDTE) to encode the multiple frame spatial details, Temporal Query Encoder (TQE) to fuse object queries, and Temporal Deformable Transformer Decoder (TDTD) to obtain current frame detection results. These designs boost the strong baseline deformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID dataset. TransVOD yields comparable results performance on the benchmark of ImageNet VID. We hope our TransVOD can provide a new perspective for video object detection.","tags":[],"title":"End-to-End Video Object Detection with Spatial-Temporal Transformers","type":"publication"},{"authors":["Xiangtai Li","Yunhai Tong"],"categories":[],"content":"","date":1634428800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634428800,"objectID":"95e33ef6449589c2186c6cc2947b5d12","permalink":"https://MSALab-PKU.github.io/publication/rdm-pgm-2021/","publishdate":"2021-10-17T00:00:00Z","relpermalink":"/publication/rdm-pgm-2021/","section":"publication","summary":"Glass-like objects such as windows, bottles, and mirrors exist widely in the real world. Sensing these objects has many applications, including robot navigation and grasping. However, this task is very challenging due to the arbitrary scenes behind glass-like objects. This paper aims to solve the glass-like object segmentation problem via enhanced boundary learning. In particular, we first propose a novel refined differential module that outputs finer boundary cues. We then introduce an edge-aware point-based graph convolution network module to model the global shape along the boundary. We use these two modules to design a decoder that generates accurate and clean segmentation results, especially on the object contours. Both modules are lightweight and effective: they can be embedded into various segmentation models. In extensive experiments on three recent glass-like object segmentation datasets, including Trans10k, MSD, and GDD, our approach establishes new state-of-the-art results. We also illustrate the strong generalization properties of our method on three generic segmentation datasets, including Cityscapes, BDD, and COCO Stuff. Code and models will be available for further research.","tags":[],"title":"Enhanced Boundary Learning for Glass-like Object Segmentation Supplementary Material","type":"publication"},{"authors":["Yiren Chen","Jiangang Bai","Yunhai Tong"],"categories":[],"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"2b6578fc61282bf970f06f051c5feb75","permalink":"https://MSALab-PKU.github.io/publication/ssa-2021/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/publication/ssa-2021/","section":"publication","summary":"One of the most popular paradigms of applying large pre-trained NLP models such as BERT is to fine-tune it on a smaller dataset. However, one challenge remains as the fine-tuned model often overfits on smaller datasets. A symptom of this phenomenon is that irrelevant or misleading words in the sentence, which are easy to understand for human beings, can substantially degrade the performance of these fine-tuned BERT models. In this paper, we propose a novel technique, called Self-Supervised Attention (SSA) to help facilitate this generalization challenge. Specifically, SSA automatically generates weak, token-level attention labels iteratively by probing the fine-tuned model from the previous iteration. We investigate two different ways of integrating SSA into BERT and propose a hybrid approach to combine their benefits. Empirically, through a variety of public datasets, we illustrate significant performance improvement using our SSA-enhanced BERT model.","tags":[],"title":"Improving BERT with Self-Supervised Attention","type":"publication"},{"authors":["Yaming Yang","Yujing Wang"],"categories":[],"content":"","date":1628899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628899200,"objectID":"aede640dcc1d04080dd8c7cd0522b842","permalink":"https://MSALab-PKU.github.io/publication/degnn-2021/","publishdate":"2021-08-14T00:00:00Z","relpermalink":"/publication/degnn-2021/","section":"publication","summary":"Mining from graph-structured data is an integral component of graph data management. A recent trending technique, graph convolutional network (GCN), has gained momentum in the graph mining field, and plays an essential part in numerous graph-related tasks. Although the emerging GCN optimization techniques bring improvements to specific scenarios, they perform diversely in different applications and introduce many trial-and-error costs for practitioners. Moreover, existing GCN models often suffer from oversmoothing problem. Besides, the entanglement of various graph patterns could lead to non-robustness and harm the final performance of GCNs. In this work, we propose a simple yet efficient graph decomposition approach to improve the performance of general graph neural networks. We first empirically study existing graph decomposition methods and propose an automatic connectivity-ware graph decomposition algorithm, DeGNN. To provide a theoretical explanation, we then characterize GCN from the information-theoretic perspective and show that under certain conditions, the mutual information between the output after *l* layers and the input of GCN converges to 0 exponentially with respect to *l*. On the other hand, we show that graph decomposition can potentially weaken the condition of such convergence rate, alleviating the information loss when GCN becomes deeper. Extensive experiments on various academic benchmarks and real-world production datasets demonstrate that graph decomposition generally boosts the performance of GNN models. Moreover, our proposed solution DeGNN achieves state-of-the-art performances on almost all these tasks.","tags":[],"title":"DeGNN: Improving Graph Neural Networks with Graph Decomposition","type":"publication"},{"authors":["Xiangtai Li","Yunhai Tong"],"categories":[],"content":"","date":1627948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627948800,"objectID":"33b7716184e2d2ef28d991bbaae4c3ee","permalink":"https://MSALab-PKU.github.io/publication/gald-2021/","publishdate":"2021-08-03T00:00:00Z","relpermalink":"/publication/gald-2021/","section":"publication","summary":"Modelling long-range contextual relationships is critical for pixel-wise prediction tasks such as semantic segmentation. However, convolutional neural networks (CNNs) are inherently limited to model such dependencies due to the naive structure in its building modules (*e.g.*, local convolution kernel). While recent global aggregation methods are beneficial for long-range structure information modelling, they would oversmooth and bring noise to the regions contain fine details (*e.g.*, boundaries and small objects), which are very much cared in the semantic segmentation task. To alleviate this problem, we propose to explore the local context for making the aggregated long-range relationship being distributed more accurately in local regions. In particular, we design a novel local distribution module which models the affinity map between global and local relationship for each pixel adaptively. Integrating existing global aggregation modules, we show that our approach can be modularized as an end-to-end trainable block and easily plugged into existing semantic segmentation networks, giving rise to the *GALD* networks. Despite its simplicity and versatility, our approach allows us to build new state of the art on major semantic segmentation benchmarks including Cityscapes, ADE20K, Pascal Context, Camvid and COCO-stuff. Code and trained models are released at https://github.com/lxtGH/GALD-DGCNet to foster further research.","tags":[],"title":"Global Aggregation then Local Distribution for Scene Parsing","type":"publication"},{"authors":["Mingliang Zhang","Yunhai Tong"],"categories":[],"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"65b716bb12fa77f3876e7d5e1fd0c128","permalink":"https://MSALab-PKU.github.io/publication/ccl-m-2021/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/publication/ccl-m-2021/","section":"publication","summary":"Currently, multilingual machine translation is receiving more and more attention since it brings better performance for low resource languages (LRLs) and saves more space. However, existing multilingual machine translation models face a severe challenge: imbalance. As a result, the translation performance of different languages in multilingual translation models are quite different. We argue that this imbalance problem stems from the different learning competencies of different languages. Therefore, we focus on balancing the learning competencies of different languages and propose ***C**ompetence-based **C**urriculum **L**earning for **M**ultilingual Machine Translation*, named CCL-M. Specifically, we firstly define two competencies to help schedule the high resource languages (HRLs) and the low resource languages: 1) *Self-evaluated Competence*, evaluating how well the language itself has been learned; and 2) *HRLs-evaluated Competence*, evaluating whether an LRL is ready to be learned according to *HRLs' Self-evaluated Competence*. Based on the above competencies, we utilize the proposed CCL-M algorithm to gradually add new languages into the training set in a curriculum learning manner. Furthermore, we propose a novel competenceaware dynamic balancing sampling strategy for better selecting training samples in multilingual training. Experimental results show that our approach has achieved a steady and significant performance gain compared to the previous state-of-the-art approach on the TED talks dataset.","tags":[],"title":"Competence-based Curriculum Learning for Multilingual Machine Translation","type":"publication"},{"authors":["Xiangtai Li","Yunhai Tong"],"categories":[],"content":"","date":1627603200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627603200,"objectID":"117dab097b4d7f74390d01e09c79c846","permalink":"https://MSALab-PKU.github.io/publication/srnet-2021/","publishdate":"2021-07-30T00:00:00Z","relpermalink":"/publication/srnet-2021/","section":"publication","summary":"Graph-based convolutional model such as non-local block has shown to be effective for strengthening the context modeling ability in convolutional neural networks (CNNs). However, its pixel-wise computational overhead is prohibitive which renders it unsuitable for high resolution imagery. In this paper, we explore the efficiency of context graph reasoning and propose a novel framework called Squeeze Reasoning. Instead of propagating information on the spatial map, we first learn to squeeze the input feature into a channel-wise global vector and perform reasoning within the single vector where the computation cost can be significantly reduced. Specifically, we build the node graph in the vector where each node represents an abstract semantic concept. The refined feature within the same semantic category results to be consistent, which is thus beneficial for downstream tasks. We show that our approach can be modularized as an end-to-end trained block and can be easily plugged into existing networks. Despite its simplicity and being lightweight, the proposed strategy allows us to establish the considerable results on different semantic segmentation datasets and shows significant improvements with respect to strong baselines on various other scene understanding tasks including object detection, instance segmentation and panoptic segmentation. Code is available at https://github.com/lxtGH/SFSegNets.","tags":[],"title":"Towards Efficient Scene Understanding via Squeeze Reasoning","type":"publication"},{"authors":["Xiangtai Li","Yunhai Tong"],"categories":[],"content":"","date":1627430400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627430400,"objectID":"d40fc09f7e98333888930fd601c16389","permalink":"https://MSALab-PKU.github.io/publication/tpr-2021/","publishdate":"2021-07-28T00:00:00Z","relpermalink":"/publication/tpr-2021/","section":"publication","summary":"Video Instance Segmentation (VIS) is a new and inherently multi-task problem, which aims to detect, segment and track each instance in a video sequence. Existing approaches are mainly based on single-frame features or single-scale features of multiple frames, where temporal information or multi-scale information is ignored. To incorporate both temporal and scale information, we propose a Temporal Pyramid Routing (TPR) strategy to conditionally align and conduct pixel-level aggregation from a feature pyramid pair of two adjacent frames. Specifically, TPR contains two novel components, including Dynamic Aligned Cell Routing (DACR) and Cross Pyramid Routing (CPR), where DACR is designed for aligning and gating pyramid features across temporal dimension, while CPR transfers temporally aggregated features across scale dimension. Moreover, our approach is a plug-and-play module and can be easily applied to existing instance segmentation methods. Extensive experiments on YouTube-VIS dataset demonstrate the effectiveness and efficiency of the proposed approach on several state-of-the-art instance segmentation methods. Codes and trained models will be publicly available to facilitate future research.(https://github.com/lxtGH/TemporalPyramidRouting).","tags":[],"title":"Improving Video Instance Segmentation via Temporal Pyramid Routing","type":"publication"},{"authors":["Yujing Wang","Yaming Yang","Jiangang Bai","Mingliang Zhang","Yunhai Tong"],"categories":[],"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"0a4736ffaea439ac8091208927255498","permalink":"https://MSALab-PKU.github.io/publication/evolvingattention-2021/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/publication/evolvingattention-2021/","section":"publication","summary":"Transformer is a ubiquitous model for natural language processing and has attracted wide attentions in computer vision. The attention maps are indispensable for a transformer model to encode the dependencies among input tokens. However, they are learned without explicit interactions in each layer and sometimes fail to capture reasonable patterns. In this paper, we propose a novel and generic mechanism based on evolving attention to improve the performance of transformers. On one hand, the attention maps in different layers share common knowledge, thus the ones in preceding layers can instruct the learning of attention in succeeding layers through residual connections. On the other hand, low-level and high-level attentions vary in the levels of abstraction, so we adopt additional convolutional layers to capture the evolutionary process of attention maps. The proposed evolving attention mechanism achieves significant performance improvement over various state-ofthe-art models for multiple tasks, including image classification, natural language understanding and machine translation.","tags":[],"title":"Evolving Attention with Residual Convolutions","type":"publication"},{"authors":["Xiangtai Li","Yunhai Tong"],"categories":[],"content":"","date":1624060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624060800,"objectID":"a4738e8454d220bc03a1dbd6d6148d0c","permalink":"https://MSALab-PKU.github.io/publication/pointflow-2021/","publishdate":"2021-06-19T00:00:00Z","relpermalink":"/publication/pointflow-2021/","section":"publication","summary":"Aerial Image Segmentation is a particular semantic segmentation problem and has several challenging characteristics that general semantic segmentation does not have. There are two critical issues: The one is an extremely foreground-background imbalanced distribution, and the other is multiple small objects along with the complex background. Such problems make the recent dense affinity context modeling perform poorly even compared with baselines due to over-introduced background context. To handle these problems, we propose a point-wise affinity propagation module based on the Feature Pyramid Network (FPN) framework, named PointFlow. Rather than dense affinity learning, a sparse affinity map is generated upon selected points between the adjacent features, which reduces the noise introduced by the background while keeping efficiency. In particular, we design a dual point matcher to select points from the salient area and object boundaries, respectively. Experimental results on three different aerial segmentation datasets suggest that the proposed method is more effective and efficient than state-of-the-art general semantic segmentation methods. Especially, our methods achieve the best speed and accuracy trade-off on three aerial benchmarks. Further experiments on three general semantic segmentation datasets prove the generality of our method. Code and models are made available (https://github.com/lxtGH/PFSegNets).","tags":[],"title":"PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation","type":"publication"},{"authors":["Yujing Wang"],"categories":[],"content":"","date":1623110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623110400,"objectID":"afc746d40770f550e6f42d7d906788c0","permalink":"https://MSALab-PKU.github.io/publication/cogtree-2021/","publishdate":"2021-06-08T00:00:00Z","relpermalink":"/publication/cogtree-2021/","section":"publication","summary":"Scene graphs are semantic abstraction of images that encourage visual understanding and reasoning. However, the performance of Scene Graph Generation (SGG) is unsatisfactory when faced with biased data in real-world scenarios. Conventional debiasing research mainly studies from the view of balancing data distribution or learning unbiased models and representations, ignoring the correlations among the biased classes. In this work, we analyze this problem from a novel cognition perspective: automatically building a hierarchical cognitive structure from the biased predictions and navigating that hierarchy to locate the relationships, making the tail relationships receive more attention in a coarse-to-fine mode. To this end, we propose a novel debiasing Cognition Tree (CogTree) loss for unbiased SGG. We first build a cognitive structure CogTree to organize the relationships based on the prediction of a biased SGG model. The CogTree distinguishes remarkably different relationships at first and then focuses on a small portion of easily confused ones. Then, we propose a debiasing loss specially for this cognitive structure, which supports coarse-to-fine distinction for the correct relationships. The loss is model-agnostic and consistently boosting the performance of several state-of-the-art models. The code is available at: https://github.com/CYVincent/SceneGraph-Transformer-CogTree.","tags":[],"title":"CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation","type":"publication"},{"authors":["Xiangtai Li","Yunhai Tong"],"categories":[],"content":"","date":1621900800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621900800,"objectID":"c324d06dd77d0752bfd2d001bc69bd82","permalink":"https://MSALab-PKU.github.io/publication/ddsm-2021/","publishdate":"2021-05-25T00:00:00Z","relpermalink":"/publication/ddsm-2021/","section":"publication","summary":"Representation of semantic context and local details is the essential issue for building modern semantic segmentation models. However, the interrelationship between semantic context and local details is not well explored in previous works. In this paper, we propose a Dynamic Dual Sampling Module (DDSM) to conduct dynamic affinity modeling and propagate semantic context to local details, which yields a more discriminative representation. Specifically, a dynamic sampling strategy is used to sparsely sample representative pixels and channels in the higher layer, forming adaptive compact support for each pixel and channel in the lower layer. The sampled features with high semantics are aggregated according to the affinities and then propagated to detailed lower-layer features, leading to a fine-grained segmentation result with wellpreserved boundaries. Experiment results on both Cityscapes and Camvid datasets validate the effectiveness and efficiency of the proposed approach. Code and models will be available at https://github.com/Fantasticarl/DDSM.","tags":[],"title":"Dynamic Dual Sampling Module For Fine-Grained Semantic Segmentation","type":"publication"},{"authors":["Xiangtai Li","Yunhai Tong"],"categories":[],"content":"","date":1621900800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621900800,"objectID":"d81cc8e250d541c9bd0f26368fbff116","permalink":"https://MSALab-PKU.github.io/publication/bialignnet-2021/","publishdate":"2021-05-25T00:00:00Z","relpermalink":"/publication/bialignnet-2021/","section":"publication","summary":"In this paper, we propose an effective method for fast and accurate scene parsing called Bidirectional Alignment Network (BiAlignNet). Previously, one representative work BiSeNet [1] uses two different paths (Context Path and Spatial Path) to achieve balanced learning of semantics and details, respectively. However, the relationship between the two paths is not well explored. We argue that both paths can benefit each other in a complementary way. Motivated by this, we propose a novel network by aligning two-path information into each other through a learned flow field. To avoid the noise and semantic gaps, we introduce a Gated Flow Alignment Module to align both features in a bidirectional way. Moreover, to make the Spatial Path learn more detailed information, we present an edge-guided hard pixel mining loss to supervise the aligned learning process. Our network achieves 80.1parcent and 78.5parcent mIoU in validation and test set of Cityscapes while running at 30 FPS with full resolution inputs. Code and models will be available at https://github.com/jojacola/BiAlignNet.","tags":[],"title":"Fast and Accurate Scene Parsing via Bi-Direction Alignment Networks","type":"publication"},{"authors":["Jiangang Bai","Yujing Wang","Yiren Chen","Yaming Yang","Yunhai Tong"],"categories":[],"content":"","date":1615075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615075200,"objectID":"e485792bbe91daf6e1e1bc6a8b47d9de","permalink":"https://MSALab-PKU.github.io/publication/syntax-bert-2021/","publishdate":"2021-03-07T00:00:00Z","relpermalink":"/publication/syntax-bert-2021/","section":"publication","summary":"Pre-trained language models like BERT achieve superior performances in various NLP tasks without explicit consideration of syntactic information. Meanwhile, syntactic information has been proved to be crucial for the success of NLP applications. However, how to incorporate the syntax trees effectively and efficiently into pre-trained Transformers is still unsettled. In this paper, we address this problem by proposing a novel framework named Syntax-BERT. This framework works in a plug-and-play mode and is applicable to an arbitrary pre-trained checkpoint based on Transformer architecture. Experiments on various datasets of natural language understanding verify the effectiveness of syntax trees and achieve consistent improvement over multiple pre-trained models, including BERT, RoBERTa, and T5. At the same time, we also made our experiment code public.","tags":[],"title":"Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://MSALab-PKU.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://MSALab-PKU.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]