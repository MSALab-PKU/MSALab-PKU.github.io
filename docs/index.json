[{"authors":["chan"],"categories":null,"content":"Biography ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"56687b7fcbaceb3c4ed6d5b35f5c4e2a","permalink":"https://MSALab-PKU.github.io/authors/chan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/chan/","section":"authors","summary":"Biography ","tags":null,"title":"Chang Han","type":"authors"},{"authors":["csun"],"categories":null,"content":"Biography I am currently a Ph.D. candidate in Intelligent Science and Technology at Peking University, focusing on advanced computational models and data analysis. My academic journey began with a Bachelor’s degree in Information Management and Information Systems from Nanjing Agricultural University, followed by a Master\u0026rsquo;s degree in Library Science from the University of Chinese Academy of Sciences. My research interests lie in the fields of recommender systems, large language models, graph data, digital libraries, and deep reinforcement learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"be26fadc8643576e89752107a6fac3ee","permalink":"https://MSALab-PKU.github.io/authors/csun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/csun/","section":"authors","summary":"Biography I am currently a Ph.D. candidate in Intelligent Science and Technology at Peking University, focusing on advanced computational models and data analysis. My academic journey began with a Bachelor’s degree in Information Management and Information Systems from Nanjing Agricultural University, followed by a Master\u0026rsquo;s degree in Library Science from the University of Chinese Academy of Sciences.","tags":null,"title":"Chao Sun","type":"authors"},{"authors":["cywang"],"categories":null,"content":"Biography I am currently a PhD Student at School of Intelligence Science and Technology, Peking University (PKU), advised by Prof. Yunhai Tong. My research interest includes computer vision and machine learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a3788e3637336a57e66336b927f9efc8","permalink":"https://MSALab-PKU.github.io/authors/cywang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/cywang/","section":"authors","summary":"Biography I am currently a PhD Student at School of Intelligence Science and Technology, Peking University (PKU), advised by Prof. Yunhai Tong. My research interest includes computer vision and machine learning.","tags":null,"title":"Chaoyang Wang","type":"authors"},{"authors":["jhmeng"],"categories":null,"content":"Biography I am currently pursuing my undergraduate degree at the School of Electronics Engineering and Computer Science, Peking University. Starting in autumn 2024, I will begin a Ph.D. program at the School of Intelligence Science and Technology, Peking University. Under the mentorship of Professor Yunhai Tong, my research primarily focus on computer vision and graph neural networks. I am deeply passionate about exploring innovative solutions within these domains and contributing to the advancement of artificial intelligence.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"adec5cfb7a28c6920b23634f788104e3","permalink":"https://MSALab-PKU.github.io/authors/jhmeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jhmeng/","section":"authors","summary":"Biography I am currently pursuing my undergraduate degree at the School of Electronics Engineering and Computer Science, Peking University. Starting in autumn 2024, I will begin a Ph.D. program at the School of Intelligence Science and Technology, Peking University.","tags":null,"title":"Jiahao Meng","type":"authors"},{"authors":["jgbai"],"categories":null,"content":"Biography ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f3a76360065a3359cd90463bea76f6dc","permalink":"https://MSALab-PKU.github.io/authors/jgbai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jgbai/","section":"authors","summary":"Biography ","tags":null,"title":"Jiangang Bai","type":"authors"},{"authors":["jzwu"],"categories":null,"content":"Biography I am Jianzong Wu (吴健宗) and I am a PhD Student at School of Intelligence Science and Technology, Peking University (PKU), advised by Prof. Yunhai Tong. Previously, I obtained my bachelor’s degree at University of Science and Technology of China (USTC).\nMy research interest focuses on multi-modal learning, including feature alignment, scene understanding and content generation. So far, I have conducted research works on referring image segmentation, open vocabulary image segmentation, text-to-image editting task, multi-modal large language models, as well as several related fields.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5a0c8c939b2c265a97fc98ef25b2275b","permalink":"https://MSALab-PKU.github.io/authors/jzwu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jzwu/","section":"authors","summary":"Biography I am Jianzong Wu (吴健宗) and I am a PhD Student at School of Intelligence Science and Technology, Peking University (PKU), advised by Prof. Yunhai Tong. Previously, I obtained my bachelor’s degree at University of Science and Technology of China (USTC).","tags":null,"title":"Jianzong Wu","type":"authors"},{"authors":["jbhu"],"categories":null,"content":"Biography ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3ed410aac5b4f052f61a64ee927c14cf","permalink":"https://MSALab-PKU.github.io/authors/jbhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jbhu/","section":"authors","summary":"Biography ","tags":null,"title":"Jingbo Hu","type":"authors"},{"authors":["mlzhang"],"categories":null,"content":"Biography ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"dfedae070636eb70976aabdd2adb501f","permalink":"https://MSALab-PKU.github.io/authors/mlzhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mlzhang/","section":"authors","summary":"Biography ","tags":null,"title":"Mingliang Zhang","type":"authors"},{"authors":["qyshi"],"categories":null,"content":"Biography ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0f97a44b7cbe240cbb6052d147ee51c8","permalink":"https://MSALab-PKU.github.io/authors/qyshi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/qyshi/","section":"authors","summary":"Biography ","tags":null,"title":"Qingyu Shi","type":"authors"},{"authors":["slxu"],"categories":null,"content":"Biography I am a PhD student at Peking University (PKU) , supervised by Professor Yunhai Tong. I also work closely with Dr.Xiangtai Li and accept his supervision. My research interests include computer vision, machine learning, and scene understanding.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"68ff3b5443a4ade8666c671627510859","permalink":"https://MSALab-PKU.github.io/authors/slxu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/slxu/","section":"authors","summary":"Biography I am a PhD student at Peking University (PKU) , supervised by Professor Yunhai Tong. I also work closely with Dr.Xiangtai Li and accept his supervision. My research interests include computer vision, machine learning, and scene understanding.","tags":null,"title":"Shilin Xu","type":"authors"},{"authors":["YoungTimmy"],"categories":null,"content":"Biography ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b21181240bd935c7972236f1a9d91a55","permalink":"https://MSALab-PKU.github.io/authors/tmyang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tmyang/","section":"authors","summary":"Biography ","tags":null,"title":"Tianmeng Yang","type":"authors"},{"authors":["xtli"],"categories":null,"content":"Biography Xiangtai is a research scientist at Tiktok, Singapore. He received the Ph.D. degree from Peking University in 2022. He worked as a Research Fellow in S-Lab, a member of the Multimedia Laboratory of NTU (MMLab@NTU) at Nanyang Technological University in 2023. His research interests include computer vision and machine learning with a focus on scene understanding, segmentation, video understanding and multi-modal learning. Several of his works have been published in top-tier conferences and journals. He serves as a regular reviewer for top-tier conferences and journals, including CVPR, ICCV, ICLR, NeurIPS, T-PAMI, and IJCV.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b15b1dd349fae079cb30852b753352c5","permalink":"https://MSALab-PKU.github.io/authors/xtli/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xtli/","section":"authors","summary":"Biography Xiangtai is a research scientist at Tiktok, Singapore. He received the Ph.D. degree from Peking University in 2022. He worked as a Research Fellow in S-Lab, a member of the Multimedia Laboratory of NTU (MMLab@NTU) at Nanyang Technological University in 2023.","tags":null,"title":"Xiangtai Li","type":"authors"},{"authors":["ymyang"],"categories":null,"content":"Biography ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a8a395d490050dea6e89bf73db842358","permalink":"https://MSALab-PKU.github.io/authors/ymyang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ymyang/","section":"authors","summary":"Biography ","tags":null,"title":"Yaming Yang","type":"authors"},{"authors":["yichen"],"categories":null,"content":"Biography ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2c3dc4301bb02fdc04677974cb73959a","permalink":"https://MSALab-PKU.github.io/authors/yrchen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yrchen/","section":"authors","summary":"Biography ","tags":null,"title":"Yiren Chen","type":"authors"},{"authors":["yjwang"],"categories":null,"content":"Biography ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b675a23079a8b1fab71d06f02d9bf8fe","permalink":"https://MSALab-PKU.github.io/authors/yjwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yjwang/","section":"authors","summary":"Biography ","tags":null,"title":"Yujing Wang","type":"authors"},{"authors":null,"categories":null,"content":"Biography Prof. Yunhai Tong currently is a Professor with the School of Artificial Intelligence, Peking University. Before that, he received his Ph.D. degree in computer science from Peking University in 2002. His main research interests include media intelligent computing, deep learning, and multimodal learning. Most of his works in computer vision, natural language processing, and machine learning, including PAMI, IJCV, EMNLP, ECCV, and CVPR, have been published in top-tier conferences and journals. He has led several national research and development projects (National Key Research and Development Program of China) since 2018.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fe6f7929557162f2fd6242ea4c7d82ee","permalink":"https://MSALab-PKU.github.io/authors/yhtong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yhtong/","section":"authors","summary":"Biography Prof. Yunhai Tong currently is a Professor with the School of Artificial Intelligence, Peking University. Before that, he received his Ph.D. degree in computer science from Peking University in 2002.","tags":null,"title":"Yunhai Tong","type":"authors"},{"authors":["zmiao"],"categories":null,"content":"Biography ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"db1f392f869907e89db7137c5bed49a2","permalink":"https://MSALab-PKU.github.io/authors/zmiao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zmiao/","section":"authors","summary":"Biography ","tags":null,"title":"Zheng Miao","type":"authors"},{"authors":["zjlin"],"categories":null,"content":"Biography ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9e45d01cc28563997db9fc47cedc0a1e","permalink":"https://MSALab-PKU.github.io/authors/zjlin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zjlin/","section":"authors","summary":"Biography ","tags":null,"title":"Zhengjie Lin","type":"authors"},{"authors":["Chaoyang Wang","Xiangtai Li","Henghui Ding","Lu Qi","Jiangning Zhang","Yunhai Tong","Chen Change Loy","Shuicheng Yan"],"categories":[],"content":"","date":1710374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710374400,"objectID":"c248cdd1062f90c656c23270a4edddac","permalink":"https://MSALab-PKU.github.io/publication/explore-2024/","publishdate":"2024-03-14T00:00:00Z","relpermalink":"/publication/explore-2024/","section":"publication","summary":"In-context segmentation has drawn more attention with the introduction of vision foundation models. Most existing approaches adopt metric learning or masked image modeling to build the correlation between visual prompts and input image queries. In this work, we explore this problem from a new perspective, using one representative generation model, the latent diffusion model (LDM). We observe a task gap between generation and segmentation in diffusion models, but LDM is still an effective minimalist for in-context segmentation. In particular, we propose two meta-architectures and correspondingly design several output alignment and optimization strategies. We have conducted comprehensive ablation studies and empirically found that the segmentation quality counts on output alignment and in-context instructions. Moreover, we build a new and fair in-context segmentation benchmark that includes both image and video datasets. Experiments validate the efficiency of our approach, demonstrating comparable or even stronger results than previous specialist models or visual foundation models. Our study shows that LDMs can also achieve good enough results for challenging in-context segmentation tasks.","tags":[],"title":"Explore In-Context Segmentation via Latent Diffusion Models","type":"publication"},{"authors":["Jianzong Wu","Xiangtai Li","Xia Li","Henghui Ding","Yunhai Tong","Dacheng Tao"],"categories":[],"content":"","date":1709596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709596800,"objectID":"b91f0528a24443e71144329712180f1a","permalink":"https://MSALab-PKU.github.io/publication/towards-robust-2024/","publishdate":"2019-09-03T00:00:00Z","relpermalink":"/publication/towards-robust-2024/","section":"publication","summary":"Referring Image Segmentation (RIS) is a fundamental vision-language task that outputs object masks based on text descriptions. Many works have achieved considerable progress for RIS, including different fusion method designs. In this work, we explore an essential question, “What if the text description is wrong or misleading?” For example, the described objects are not in the image. We term such a sentence as a negative sentence. However, existing solutions for RIS cannot handle such a setting. To this end, we propose a new formulation of RIS, named Robust Referring Image Segmentation (R-RIS). It considers the negative sentence inputs besides the regular positive text inputs. To facilitate this new task, we create three R-RIS datasets by augmenting existing RIS datasets with negative sentences and propose new metrics to evaluate both types of inputs in a unified manner. Furthermore, we propose a new transformer-based model, called RefSegformer, with a token-based vision and language fusion module. Our design can be easily extended to our R-RIS setting by adding extra blank tokens. Our proposed RefSegformer achieves state-of-the-art results on both RIS and R-RIS datasets, establishing a solid baseline for both settings.","tags":[],"title":"Towards robust referring image segmentation","type":"publication"},{"authors":["Xiangtai Li","Jiangning Zhang","Yibo Yang","Guangliang Cheng","Kuiyuan Yang","Yunhai Tong","Dacheng Tao"],"categories":[],"content":"","date":1708214400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708214400,"objectID":"e7958d15f881013b3512b6f2e2af9026","permalink":"https://MSALab-PKU.github.io/publication/sfnet-2024/","publishdate":"2024-02-18T00:00:00Z","relpermalink":"/publication/sfnet-2024/","section":"publication","summary":"In this paper, we focus on exploring effective methods for faster and accurate semantic segmentation. A common practice to improve the performance is to attain high-resolution feature maps with strong semantic representation. Two strategies are widely used: atrous convolutions and feature pyramid fusion, while both are either computationally intensive or ineffective. Inspired by the Optical Flow for motion alignment between adjacent video frames, we propose a Flow Alignment Module (FAM) to learn Semantic Flow between feature maps of adjacent levels and broadcast high-level features to high-resolution features effectively and efficiently. Furthermore, integrating our FAM to a standard feature pyramid structure exhibits superior performance over other real-time methods, even on lightweight backbone networks, such as ResNet-18 and DFNet. Then to further speed up the inference procedure, we also present a novel Gated Dual Flow Alignment Module to directly align high-resolution feature maps and low-resolution feature maps where we term the improved version network as SFNet-Lite. Extensive experiments are conducted on several challenging datasets, where results show the effectiveness of both SFNet and SFNet-Lite. In particular, when using Cityscapes test set, the SFNet-Lite series achieve 80.1 mIoU while running at 60 FPS using ResNet-18 backbone and 78.8 mIoU while running at 120 FPS using STDC backbone on RTX-3090. Moreover, we unify four challenging driving datasets (i.e., Cityscapes, Mapillary, IDD, and BDD) into one large dataset, which we named Unified Driving Segmentation (UDS) dataset. It contains diverse domain and style information. We benchmark several representative works on UDS. Both SFNet and SFNet-Lite still achieve the best speed and accuracy trade-off on UDS, which serves as a strong baseline in such a challenging setting. The code and models are publicly available at https://github.com/lxtGH/SFSegNets.","tags":[],"title":"Sfnet: Faster and accurate semantic segmentation via semantic flow","type":"publication"},{"authors":["Jianzong Wu","Xiangtai Li","Shilin Xu","Haobo Yuan","Henghui Ding","Yibo Yang","Xia Li","Jiangning Zhang","Yunhai Tong","Xudong Jiang","Bernard Ghanem","Dacheng Tao"],"categories":[],"content":"","date":1707091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707091200,"objectID":"ea0bf70f3f2fe2274e75c74f7c75aef9","permalink":"https://MSALab-PKU.github.io/publication/towards-2024/","publishdate":"2024-02-05T00:00:00Z","relpermalink":"/publication/towards-2024/","section":"publication","summary":"In the field of visual scene understanding, deep neural networks have made impressive advancements in various core tasks like segmentation, tracking, and detection. However, most approaches operate on the close-set assumption, meaning that the model can only identify pre-defined categories that are present in the training set. Recently, open vocabulary settings were proposed due to the rapid progress of vision language pre-training. These new approaches seek to locate and recognize categories beyond the annotated label space. The open vocabulary approach is more general, practical, and effective than weakly supervised and zero-shot settings. This paper thoroughly reviews open vocabulary learning, summarizing and analyzing recent developments in the field. In particular, we begin by juxtaposing open vocabulary learning with analogous concepts such as zero-shot learning, open-set recognition, and out-of-distribution detection. Subsequently, we examine several pertinent tasks within the realms of segmentation and detection, encompassing long-tail problems, few-shot, and zero-shot settings. As a foundation for our method survey, we first elucidate the fundamental principles of detection and segmentation in close-set scenarios. Next, we examine various contexts where open vocabulary learning is employed, pinpointing recurring design elements and central themes. This is followed by a comparative analysis of recent detection and segmentation methodologies in commonly used datasets and benchmarks. Our review culminates with a synthesis of insights, challenges, and discourse on prospective research trajectories. To our knowledge, this constitutes the inaugural exhaustive literature review on open vocabulary learning. We keep tracing related works at https://github.com/jianzongwu/Awesome-Open-Vocabulary.","tags":[],"title":"Towards open vocabulary learning: A survey","type":"publication"},{"authors":["Shilin Xu","Haobo Yuan","Qingyu Shi","Lu Qi","Jingbo Wang","Yibo Yang","Yining Li","Kai Chen","Yunhai Tong","Bernard Ghanem","Xiangtai Li","Ming-Hsuan Yang"],"categories":[],"content":"","date":1705536000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705536000,"objectID":"269a019d5b7797e3e0af8a2499c95eda","permalink":"https://MSALab-PKU.github.io/publication/rap-2024/","publishdate":"2024-01-18T00:00:00Z","relpermalink":"/publication/rap-2024/","section":"publication","summary":"Advanced by transformer architecture, vision foundation models (VFMs) achieve remarkable progress in performance and generalization ability. Segment Anything Model (SAM) is one remarkable model that can achieve generalized segmentation. However, most VFMs cannot run in realtime, which makes it difficult to transfer them into several products. On the other hand, current real-time segmentation mainly has one purpose, such as semantic segmentation on the driving scene. We argue that diverse outputs are needed for real applications. Thus, this work explores a new real-time segmentation setting, named all-purpose segmentation in real-time, to transfer VFMs in real-time deployment. It contains three different tasks, including interactive segmentation, panoptic segmentation, and video segmentation. We aim to use one model to achieve the above tasks in real-time. We first benchmark several strong baselines. Then, we present Real-Time All Purpose SAM (RAP-SAM). It contains an efficient encoder and an efficient decoupled decoder to perform prompt-driven decoding. Moreover, we further explore different training strategies and tuning methods to boost co-training performance further. Our code and model are available at https://github.com/xushilin1/RAP-SAM/.","tags":[],"title":"RAP-SAM: Towards Real-Time All-Purpose Segment Anything","type":"publication"},{"authors":["Jianzong Wu","Xiangtai Li","Chenyang Si","Shangchen Zhou","Jingkang Yang","Jiangning Zhang","Yining Li","Kai Chen","Yunhai Tong","Ziwei Liu","Chen Change Loy"],"categories":[],"content":"","date":1705536000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705536000,"objectID":"5b8dc36e3dce63bd259d3a8f3ead5f9f","permalink":"https://MSALab-PKU.github.io/publication/language-driven-2024/","publishdate":"2024-01-18T00:00:00Z","relpermalink":"/publication/language-driven-2024/","section":"publication","summary":"We introduce a new task -- language-driven video inpainting, which uses natural language instructions to guide the inpainting process. This approach overcomes the limitations of traditional video inpainting methods that depend on manually labeled binary masks, a process often tedious and labor-intensive. We present the Remove Objects from Videos by Instructions (ROVI) dataset, containing 5,650 videos and 9,091 inpainting results, to support training and evaluation for this task. We also propose a novel diffusion-based language-driven video inpainting framework, the first end-to-end baseline for this task, integrating Multimodal Large Language Models to understand and execute complex language-based inpainting requests effectively. Our comprehensive results showcase the dataset's versatility and the model's effectiveness in various language-instructed inpainting scenarios. We will make datasets, code, and models publicly available.","tags":[],"title":"Towards language-driven video inpainting via multimodal large language models","type":"publication"},{"authors":["Tianmeng Yang","Min Zhou","Yujing Wang","Zhengjie Lin","Lujia Pan","Bin Cui","Yunhai Tong"],"categories":[],"content":"","date":1697846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697846400,"objectID":"0dceb85ea12a5eb514de1e833bc1e018","permalink":"https://MSALab-PKU.github.io/publication/mitigating-2023/","publishdate":"2023-10-21T00:00:00Z","relpermalink":"/publication/mitigating-2023/","section":"publication","summary":"Graph Active Learning (GAL), which aims to find the most informative nodes in graphs for annotation to maximize the Graph Neural Networks (GNNs) performance, has attracted many research efforts but remains non-trivial challenges. One major challenge is that existing GAL strategies may introduce semantic confusion to the selected training set, particularly when graphs are noisy. Specifically, most existing methods assume all aggregating features to be helpful, ignoring the semantically negative effect between inter-class edges under the message-passing mechanism. In this work, we present Semantic-aware Active learning framework for Graphs (SAG) to mitigate the semantic confusion problem. Pairwise similarities and dissimilarities of nodes with semantic features are introduced to jointly evaluate the node influence. A new prototypebased criterion and query policy are also designed to maintain diversity and class balance of the selected nodes, respectively. Extensive experiments on the public benchmark graphs and a real-world financial dataset demonstrate that SAG significantly improves node classification performances and consistently outperforms previous methods. Moreover, comprehensive analysis and ablation study also verify the effectiveness of the proposed framework.","tags":[],"title":"Mitigating Semantic Confusion from Hostile Neighborhood for Graph Active Learning","type":"publication"},{"authors":["Shilin Xu","Xiangtai Li","Size Wu","Wenwei Zhang","Yining Li","Guangliang Cheng","Yunhai Tong","Kai Chen","Chen Change Loy"],"categories":[],"content":"","date":1696204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696204800,"objectID":"9d133eb05a2344f8145e9a44a8309635","permalink":"https://MSALab-PKU.github.io/publication/dst-2024/","publishdate":"2023-10-02T00:00:00Z","relpermalink":"/publication/dst-2024/","section":"publication","summary":"Open-vocabulary object detection (OVOD) aims to detect the objects beyond the set of categories observed during training. This work presents a simple yet effective strategy that leverages the zero-shot classification ability of pre-trained vision-language models (VLM), such as CLIP, to classify proposals for all possible novel classes directly. Unlike previous works that ignore novel classes during training and rely solely on the region proposal network (RPN) for novel object detection, our method selectively filters proposals based on specific design criteria. The resulting sets of identified proposals serve as pseudo-labels for novel classes during the training phase. It enables our self-training strategy to improve the recall and accuracy of novel classes in a self-training manner without requiring additional annotations or datasets. We further propose a simple offline pseudo-label generation strategy to refine the object detector. Empirical evaluations on three datasets, including LVIS, V3Det, and COCO, demonstrate significant improvements over the baseline performance without incurring additional parameters or computational costs during inference. In particular, compared with previous F-VLM, our method achieves a 1.7-2.0% improvement on LVIS dataset and 2.3-3.8% improvement on the recent challenging V3Det dataset. Our method also boosts the strong baseline by 6% mAP on COCO. The code and models will be publicly available at https://github.com/xushilin1/dst-det.","tags":[],"title":"Dst-det: Simple dynamic self-training for open-vocabulary object detection","type":"publication"},{"authors":["Yaming Yang","Jieyu Zhang","Yujing Wang","Zheng Miao","Yunhai Tong"],"categories":[],"content":"","date":1694649600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694649600,"objectID":"3fe970d5f967f967c5b36a65e7664fd9","permalink":"https://MSALab-PKU.github.io/publication/multiple-2023/","publishdate":"2023-09-14T00:00:00Z","relpermalink":"/publication/multiple-2023/","section":"publication","summary":"Session-based recommendation (SBR), which makes the next-item recommendation based on previous anonymous actions, has drawn increasing attention. The last decade has seen multiple deep learning-based modeling choices applied on SBR successfully, e.g., recurrent neural networks (RNNs), convolutional neural networks (CNNs), graph neural networks (GNNs), and each modeling choice has its intrinsic superiority and limitation. We argue that these modeling choices differentiate from each other by (1) the way they capture the interactions between items within a session and (2) the operators they adopt for composing the neural network, e.g., convolutional operator or self-attention operator. In this work, we dive deep into the former as it is relatively unique to the SBR scenario, while the latter is shared by general neural network modeling techniques. We first introduce the concept of connectivity view to describe the different item interaction patterns at the input level. Then, we develop the Multiple Connectivity Views for Session-based Recommendation (MCV-SBR), a unified framework that incorporates different modeling choices in a single model through the lens of connectivity view. In addition, MCV-SBR allows us to effectively and efficiently explore the search space of the combinations of connectivity views by the Tree-structured Parzen Estimator (TPE) algorithm. Finally, on three widely used SBR datasets, we verify the superiority of MCV-SBR by comparing the searched models with state-of-the-art baselines. We also conduct a series of studies to demonstrate the efficacy and practicability of the proposed connectivity view search algorithm, as well as other components in MCV-SBR.","tags":[],"title":"Multiple Connectivity Views for Session-based Recommendation","type":"publication"},{"authors":["Yangyang Xu","Xiangtai Li","Haobo Yuan","Yibo Yang","Lefei Zhang"],"categories":[],"content":"","date":1688688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688688000,"objectID":"cce25c86bfb6b34901ebb871d5ae4dfd","permalink":"https://MSALab-PKU.github.io/publication/multi-task-2023/","publishdate":"2023-07-07T00:00:00Z","relpermalink":"/publication/multi-task-2023/","section":"publication","summary":"Previous multi-task dense prediction studies developed complex pipelines such as multi-modal distillations in multiple stages or searching for task relational contexts for each task. The core insight beyond these methods is to maximize the mutual effects of each task. Inspired by the recent querybased Transformers, we propose a simple pipeline named MultiQuery Transformer (MQTransformer) that is equipped with multiple queries from different tasks to facilitate the reasoning among multiple tasks and simplify the cross-task interaction pipeline. Instead of modeling the dense per-pixel context among different tasks, we seek a task-specific proxy to perform crosstask reasoning via multiple queries where each query encodes the task-related context. The MQTransformer is composed of three key components: shared encoder, cross-task query attention module and shared decoder. We first model each task with a task-relevant query. Then both the task-specific feature output by the feature extractor and the task-relevant query are fed into the shared encoder, thus encoding the task-relevant query from the task-specific feature. Secondly, we design a cross-task query attention module to reason the dependencies among multiple task-relevant queries; this enables the module to only focus on the query-level interaction. Finally, we use a shared decoder to gradually refine the image features with the reasoned query features from different tasks. Extensive experiment results on two dense prediction datasets (NYUD-v2 and PASCAL-Context) show that the proposed method is an effective approach and achieves state-of-the-art results. Code and models are available at https://github.com/yangyangxu0/MQTransformer.","tags":[],"title":"Multi-task learning with multi-query transformer for dense prediction","type":"publication"},{"authors":["Jianzong Wu","Xiangtai Li","Henghui Ding","Xia Li","Guangliang Cheng","Yunhai Tong","Chen Change Loy"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688169600,"objectID":"565db746f149f9fe56355788471cd56a","permalink":"https://MSALab-PKU.github.io/publication/betrayed-2023/","publishdate":"2023-07-01T00:00:00Z","relpermalink":"/publication/betrayed-2023/","section":"publication","summary":"In this work, we focus on open vocabulary instance segmentation to expand a segmentation model to classify and segment instance-level novel categories. Previous approaches have relied on massive caption datasets and complex pipelines to establish one-to-one mappings between image regions and words in captions. However, such methods build noisy supervision by matching non-visible words to image regions, such as adjectives and verbs. Meanwhile, context words are also important for inferring the existence of novel objects as they show high inter-correlations with novel categories. To overcome these limitations, we devise a joint Caption Grounding and Generation (CGG) framework, which incorporates a novel grounding loss that only focuses on matching object nouns to improve learning efficiency. We also introduce a caption generation head that enables additional supervision and contextual modeling as a complementation to the grounding loss. Our analysis and results demonstrate that grounding and generation components complement each other, significantly enhancing the segmentation performance for novel classes. Experiments on the COCO dataset with two settings: Open Vocabulary Instance Segmentation (OVIS) and Open Set Panoptic Segmentation (OSPS) demonstrate the superiority of the CGG. Specifically, CGG achieves a substantial improvement of 6.8% mAP for novel classes without extra data on the OVIS task and 15% PQ improvements for novel classes on the OSPS benchmark.","tags":[],"title":"Betrayed by captions: Joint caption grounding and generation for open vocabulary instance segmentation","type":"publication"},{"authors":["Xiangtai Li","Shilin Xu","Yibo Yang","Haobo Yuan","Guangliang Cheng","Yunhai Tong","Zhouchen Lin","Ming-Hsuan Yang","Dacheng Tao"],"categories":[],"content":"","date":1679443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679443200,"objectID":"3cd5292d206df6b9ad24abebb595b486","permalink":"https://MSALab-PKU.github.io/publication/panopticpartformer++-2022/","publishdate":"2023-03-22T00:00:00Z","relpermalink":"/publication/panopticpartformer++-2022/","section":"publication","summary":"Panoptic Part Segmentation (PPS) unifies panoptic and part segmentation into one task. Previous works utilize separate approaches to handle things, stuff, and part predictions without shared computation and task association. We aim to unify these tasks at the architectural level, designing the first end-to-end unified framework, Panoptic-PartFormer. Moreover, we find the previous metric PartPQ biases to PQ. To handle both issues, we first design a meta-architecture that decouples part features and things/stuff features, respectively. We model things, stuff, and parts as object queries and directly learn to optimize all three forms of prediction as a unified mask prediction and classification problem. We term our model as Panoptic-PartFormer. Second, we propose a new metric Part-Whole Quality (PWQ), better to measure this task from pixel-region and part-whole perspectives. It also decouples the errors for part segmentation and panoptic segmentation. Third, inspired by Mask2Former, based on our meta-architecture, we propose Panoptic-PartFormer++ and design a new part-whole cross-attention scheme to boost part segmentation qualities further. We design a new part-whole interaction method using masked cross attention. Finally, extensive ablation studies and analysis demonstrate the effectiveness of both Panoptic-PartFormer and Panoptic-PartFormer++. Compared with previous Panoptic-PartFormer, our Panoptic-PartFormer++ achieves 2% PartPQ and 3% PWQ improvements on the Cityscapes PPS dataset and 5% PartPQ on the Pascal Context PPS dataset. On both datasets, Panoptic-PartFormer++ achieves new state-of-the-art results. Our models can serve as a strong baseline and aid future research in PPS. The source code and trained models will be available at https://github.com/lxtGH/Panoptic-PartFormer.","tags":[],"title":"Panopticpartformer++: A unified and decoupled view for panoptic part segmentation","type":"publication"},{"authors":["Yujing Wang","Yaming Yang","Zhuo Li","Jiangang Bai","Mingliang Zhang","Xiangtai Li","Jing Yu","Ce Zhang","Gao Huang","Yunhai Tong"],"categories":[],"content":"","date":1673568000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673568000,"objectID":"faf93c2411edd01a6dfa3ae99761e40b","permalink":"https://MSALab-PKU.github.io/publication/conv-2023/","publishdate":"2023-01-13T00:00:00Z","relpermalink":"/publication/conv-2023/","section":"publication","summary":"Attention-based neural networks, such as Transformers, have become ubiquitous in numerous applications, including computer vision, natural language processing, and time-series analysis. In all kinds of attention networks, the attention maps are crucial as they encode semantic dependencies between input tokens. However, most existing attention networks perform modeling or reasoning based on representations, wherein the attention maps of different layers are learned separately without explicit interactions. In this paper, we propose a novel and generic evolving attention mechanism, which directly models the evolution of inter-token relationships through a chain of residual convolutional modules. The major motivations are twofold. On the one hand, the attention maps in different layers share transferable knowledge, thus adding a residual connection can facilitate the information flow of inter-token relationships across layers. On the other hand, there is naturally an evolutionary trend among attention maps at different abstraction levels, so it is beneficial to exploit a dedicated convolution-based module to capture this process. Equipped with the proposed mechanism, the convolution-enhanced evolving attention networks achieve superior performance in various applications, including time-series representation, natural language understanding, machine translation, and image classification. Especially on time-series representation tasks, Evolving Attention-enhanced Dilated Convolutional (EA-DC-) Transformer outperforms state-of-the-art models significantly, achieving an average of 17% improvement compared to the best SOTA. To the best of our knowledge, this is the first work that explicitly models the layer-wise evolution of attention maps. Our implementation is available at https://github.com/pkuyym/EvolvingAttention.","tags":[],"title":"Convolution-enhanced evolving attention networks","type":"publication"},{"authors":["Hong Guo","Yujing Wang","Jieyu Zhang","Zhengjie Lin","Yunhai Tong","Lei Yang","Luoxing Xiong","Congrui Huang"],"categories":[],"content":"","date":1672358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672358400,"objectID":"870b226276ba8655ca68fdcee966e4ba","permalink":"https://MSALab-PKU.github.io/publication/label-2022/","publishdate":"2022-12-30T00:00:00Z","relpermalink":"/publication/label-2022/","section":"publication","summary":"Time-series anomaly detection is an important task and has been widely applied in the industry. Since manual data annotation is expensive and inefficient, most applications adopt unsupervised anomaly detection methods, but the results are usually sub-optimal and unsatisfactory to end customers. Weak supervision is a promising paradigm for obtaining considerable labels in a low-cost way, which enables the customers to label data by writing heuristic rules rather than annotating each instance individually. However, in the time-series domain, it is hard for people to write reasonable labeling functions as the time-series data is numerically continuous and difficult to be understood. In this paper, we propose a Label-Efficient Interactive Time-Series Anomaly Detection (LEIAD) system, which enables a user to improve the results of unsupervised anomaly detection by performing only a small amount of interactions with the system. To achieve this goal, the system integrates weak supervision and active learning collaboratively while generating labeling functions automatically using only a few labeled data. All of these techniques are complementary and can promote each other in a reinforced manner. We conduct experiments on three time-series anomaly detection datasets, demonstrating that the proposed system is superior to existing solutions in both weak supervision and active learning areas. Also, the system has been tested in a real scenario in industry to show its practicality","tags":[],"title":"Label-efficient interactive time-series anomaly detection","type":"publication"},{"authors":["Qianyu Zhou","Xiangtai Li","Lu He","Yibo Yang","Guangliang Cheng","Yunhai Tong","Lizhuang Ma","Dacheng Tao"],"categories":[],"content":"","date":1669161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669161600,"objectID":"36c92285ccea66aed0800c08b150cdaf","permalink":"https://MSALab-PKU.github.io/publication/transvod-2022/","publishdate":"2022-11-23T00:00:00Z","relpermalink":"/publication/transvod-2022/","section":"publication","summary":"Detection Transformer (DETR) and Deformable DETR have been proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance as previous complex hand-crafted detectors. However, their performance on Video Object Detection (VOD) has *not* been well explored. In this paper, we present **TransVOD**, the first end-to-end video object detection system based on spatial-temporal Transformer architectures. The first goal of this paper is to streamline the pipeline of VOD, effectively removing the need for many hand-crafted components for feature aggregation, *e.g.*, optical flow model, relation networks. Besides, benefited from the object query design in DETR, our method does not need complicated post-processing methods such as Seq-NMS. In particular, we present a temporal Transformer to aggregate both the spatial object queries and the feature memories of each frame. Our temporal transformer consists of two components: Temporal Query Encoder (TQE) to fuse object queries, and Temporal Deformable Transformer Decoder (TDTD) to obtain current frame detection results. These designs boost the strong baseline deformable DETR by a significant margin (3 %-4 % mAP) on the ImageNet VID dataset. TransVOD yields comparable performances on the benchmark of ImageNet VID. Then, we present two improved versions of TransVOD including TransVOD++ and TransVOD Lite. The former fuses object-level information into object query via dynamic convolution while the latter models the entire video clips as the output to speed up the inference time. We give detailed analysis of all three models in the experiment part. In particular, our proposed TransVOD++ sets a new state-of-the-art record in terms of accuracy on ImageNet VID with 90.0 % mAP. Our proposed TransVOD Lite also achieves the best speed and accuracy trade-off with 83.7 % mAP while running at around 30 FPS on a single V100 GPU device. Code and models will be available for further research.","tags":[],"title":"TransVOD: End-to-end Video Object Detection with Spatial-Temporal Transformers","type":"publication"},{"authors":["Xiangtai Li","Shilin Xu","Yibo Yang","Guangliang Cheng","Yunhai Tong","Dacheng Tao"],"categories":[],"content":"","date":1666483200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666483200,"objectID":"849dcf2b7b13973ab079462eb8e285d5","permalink":"https://MSALab-PKU.github.io/publication/partformer-2022/","publishdate":"2022-10-23T00:00:00Z","relpermalink":"/publication/partformer-2022/","section":"publication","summary":"Panoptic Part Segmentation (PPS) aims to unify panoptic segmentation and part segmentation into one task. Previous work mainly utilizes separated approaches to handle thing, stuff, and part predictions individually without performing any shared computation and task association. In this work, we aim to unify these tasks at the architectural level, designing the first end-to-end unified method named PanopticPartFormer. In particular, motivated by the recent progress in Vision Transformer, we model things, stuff, and part as object queries and directly learn to optimize the all three predictions as unified mask prediction and classification problem. We design a decoupled decoder to generate part feature and thing/stuff feature respectively. Then we propose to utilize all the queries and corresponding features to perform reasoning jointly and iteratively. The final mask can be obtained via inner product between queries and the corresponding features. The extensive ablation studies and analysis prove the effectiveness of our framework. Our Panoptic-PartFormer achieves the new state-of-the-art results on both Cityscapes PPS and Pascal Context PPS datasets with around 70% GFlops and 50% parameters decrease. Given its effectiveness and conceptual simplicity, we hope the Panoptic-PartFormer can serve as a strong baseline and aid future research in PPS. Our code and models will be available at https://github.com/lxtGH/Panoptic-PartFormer.","tags":[],"title":"Panoptic-partformer: Learning a unified model for panoptic part segmentation","type":"publication"},{"authors":["Haobo Yuan","Xiangtai Li","Yibo Yang","Guangliang Cheng","Jing Zhang","Yunhai Tong","Lefei Zhang","Dacheng Tao"],"categories":[],"content":"","date":1666483200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666483200,"objectID":"06d01efe12ec6aaedb1810f7c31607ca","permalink":"https://MSALab-PKU.github.io/publication/polyphonicofrmer-2022/","publishdate":"2022-10-23T00:00:00Z","relpermalink":"/publication/polyphonicofrmer-2022/","section":"publication","summary":"The Depth-aware Video Panoptic Segmentation (DVPS) is a new challenging vision problem that aims to predict panoptic segmentation and depth in a video simultaneously. The previous work solves this task by extending the existing panoptic segmentation method with an extra dense depth prediction and instance tracking head. However, the relationship between the depth and panoptic segmentation is not well explored – simply combining existing methods leads to competition and needs carefully weight balancing. In this paper, we present PolyphonicFormer, a vision transformer to unify these sub-tasks under the DVPS task and lead to more robust results. Our principal insight is that the depth can be harmonized with the panoptic segmentation with our proposed new paradigm of predicting instance level depth maps with object queries. Then the relationship between the two tasks via query-based learning is explored. From the experiments, we demonstrate the benefits of our design from both depth estimation and panoptic segmentation aspects. Since each thing query also encodes the instance-wise information, it is natural to perform tracking directly with appearance learning. Our method achieves state-of-the-art results on two DVPS datasets (Semantic KITTI, Cityscapes), and ranks 1st on the ICCV-2021 BMTT Challenge video + depth track.","tags":[],"title":"Polyphonicformer: Unified query learning for depth-aware video panoptic segmentation","type":"publication"},{"authors":["Shilin Xu","Xiangtai Li","Jingbo Wang","Guangliang Cheng","Yunhai Tong","Dacheng Tao"],"categories":[],"content":"","date":1666396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666396800,"objectID":"bbe6c87de6299da2b73129897434ea33","permalink":"https://MSALab-PKU.github.io/publication/fashionformer-2022/","publishdate":"2022-10-22T00:00:00Z","relpermalink":"/publication/fashionformer-2022/","section":"publication","summary":"Human fashion understanding is one crucial computer vision task since it has comprehensive information for real-world applications. This focus on joint human fashion segmentation and attribute recognition. Contrary to the previous works that separately model each task as a multi-head prediction problem, our insight is to bridge these two tasks with one unified model via vision transformer modeling to benefit each task. In particular, we introduce the object query for segmentation and the attribute query for attribute prediction. Both queries and their corresponding features can be linked via mask prediction. Then we adopt a two-stream query learning framework to learn the decoupled query representations.We design a novel Multi-Layer Rendering module for attribute stream to explore more fine-grained features. The decoder design shares the same spirit as DETR. Thus we name the proposed method Fahsionformer. Extensive experiments on three human fashion datasets illustrate the effectiveness of our approach. In particular, our method with the same backbone achieve relative 10% improvements than previous works in case of a joint metric (APmask IoU+F 1 ) for both segmentation and attribute recognition. To the best of our knowledge, we are the first unified end-to-end vision transformer framework for human fashion analysis. We hope this simple yet effective method can serve as a new flexible baseline for fashion analysis. Code will be available https://github.com/xushilin1/FashionFormer.","tags":[],"title":"Fashionformer: A simple, effective and unified baseline for human fashion segmentation and recognition","type":"publication"},{"authors":["Shilin Xu","Xiangtai Li","Yibo Yang","Hongyang Li","Guangliang Cheng","Yunhai Tong"],"categories":[],"content":"","date":1666051200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666051200,"objectID":"33504d2a01ac066558dd43aafa3b3777","permalink":"https://MSALab-PKU.github.io/publication/query-2022/","publishdate":"2022-10-18T00:00:00Z","relpermalink":"/publication/query-2022/","section":"publication","summary":"Starting from DETR, query based detection and segmentation methods achieve comparable results as previous works with a simplified and elegant pipeline. In this work, a novel, simple and unified baseline, named QueryPanSeg, is proposed for panoptic segmentation. QueryPanSeg represents both things and stuff as learnable queries separately. For thing query, we propose to encode each instance mask into compact mask vectors and perform classification, box regression and mask encoding regression simultaneously. For stuff query, we propose a residual interactive learning where each stuff query is responsible for one semantic category and performs pixel interaction via one multi-head attention layer. With this approach, instance-wise and semantically consistent properties for things and stuff can be unified in one framework. Compared with the original DETR, our approach results in a nearly 10 times shorter training schedule to converge. Compared with previous box-based and box-free methods, our proposed approach outperforms many state-of-the-art results with much simpler pipeline without handcrafted components.","tags":[],"title":"Query Learning of Both Thing and Stuff for Panoptic Segmentation","type":"publication"},{"authors":["Xiangtai Li","Hao He","Yibo Yang","Henghui Ding","Kuiyuan Yang","Guangliang Cheng","Yunhai Tong","Dacheng Tao"],"categories":[],"content":"","date":1664841600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664841600,"objectID":"604a7b409a900a180dd26bd3a3180e11","permalink":"https://MSALab-PKU.github.io/publication/inproving-2022/","publishdate":"2022-10-04T00:00:00Z","relpermalink":"/publication/inproving-2022/","section":"publication","summary":"Video Instance Segmentation (VIS) is a new and inherently multi-task problem, which aims to detect, segment and track each instance in a video sequence. Existing approaches are mainly based on single-frame features or single-scale features of multiple frames, where temporal information or multi-scale information is ignored. To incorporate both temporal and scale information, we propose a Temporal Pyramid Routing (TPR) strategy to conditionally align and conduct pixel-level aggregation from a feature pyramid pair of two adjacent frames. Specifically, TPR contains two novel components, including Dynamic Aligned Cell Routing (DACR) and Cross Pyramid Routing (CPR), where DACR is designed for aligning and gating pyramid features across temporal dimension, while CPR transfers temporally aggregated features across scale dimension. Moreover, our approach is a plug-and-play module and can be easily applied to existing instance segmentation methods. Extensive experiments on YouTube-VIS dataset demonstrate the effectiveness and efficiency of the proposed approach on several state-of-the-art instance segmentation methods. Codes and trained models will be publicly available to facilitate future research.(https://github.com/lxtGH/TemporalPyramidRouting).","tags":[],"title":"Improving Video Instance Segmentation via Temporal Pyramid Routing","type":"publication"},{"authors":["Jiangang Bai","Yujing Wang","Hong Sun","Ruonan Wu","Tianmeng Yang","Pengfei Tang","Defu Cao","Mingliang Zhang","Yunhai Tong","Yaming Yang","Jing Bai","Ruofei Zhang","Hao Sun","Wei Shen"],"categories":[],"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"19fe1165d0dc22317a3923cacbbb9ca3","permalink":"https://MSALab-PKU.github.io/publication/enhancing-2022/","publishdate":"2022-07-01T00:00:00Z","relpermalink":"/publication/enhancing-2022/","section":"publication","summary":"Large-scale pre-trained language models have attracted extensive attentions in the research community and shown promising results on various tasks of natural language processing. However, the attention maps, which record the attention scores between tokens in self-attention mechanism, are sometimes ineffective as they are learned implicitly without the guidance of explicit semantic knowledge. Thus, we aim to infuse explicit external knowledge into pre-trained language models to further boost their performance. Existing works of knowledge infusion largely depend on multi-task learning frameworks, which are inefficient and require large-scale re-training when new knowledge is considered. In this paper, we propose a novel and generic solution, KAM-BERT, which directly incorporates knowledge-generated attention maps into the self-attention mechanism. It requires only a few extra parameters and supports efficient fine-tuning once new knowledge is added. KAM-BERT achieves consistent improvements on various academic datasets for natural language understanding. It also outperforms other state-of-the-art methods which conduct knowledge infusion into transformer-based architectures. Moreover, we apply our model to an industry-scale ad relevance application and show its advantages in the real-world scenario.","tags":[],"title":"Enhancing self-attention with knowledge-assisted attention maps","type":"publication"},{"authors":["Yujing Wang","Luoxin Xiong","Mingliang Zhang","Hui Xue","Qi Chen","Yaming Yang","Yunhai Tong","Congrui Huang","Bixiong Xu"],"categories":[],"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"88d572276abe48535f2ef38da4b6bc03","permalink":"https://MSALab-PKU.github.io/publication/heat-rl-2022/","publishdate":"2022-07-01T00:00:00Z","relpermalink":"/publication/heat-rl-2022/","section":"publication","summary":"Time-series anomaly detection plays an important role in various applications. In a commercial system, anomaly detection models are either unsupervised or pre-trained in a self-supervised manner offline; while in the online serving stage, an appropriate model should be selected to fulfill each customer’s requirement with only a few human interactions. Existing online model selection methods do not have good data efficiency, failing to achieve good performance with limited number of manual feedbacks. In this paper, we propose Heat-RL, a novel reinforcement learning algorithm tailored to online model selection for streaming time-series data. Specifically, we design a new state based on metric-oriented heatmaps and apply ResNet for policy and value networks to capture the correlations among similar model configurations. Experiments demonstrated the effectiveness of Heat-RL on both academic and industrial datasets. On all datasets, the average F1 and last F1 scores have been improved by 5.5% and 14.6% respectively compared to the best state-of-the-art solution.","tags":[],"title":"Heat-RL: Online Model Selection for Streaming Time-Series Anomaly Detection","type":"publication"},{"authors":["Tianmeng Yang","Yujing Wang","Zhihan Yue","Yaming Yang","Yunhai Tong","Jing Bai"],"categories":[],"content":"","date":1656374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656374400,"objectID":"549ea3a7c30d85b0fd80aad695023483","permalink":"https://MSALab-PKU.github.io/publication/graph-2022/","publishdate":"2022-06-28T00:00:00Z","relpermalink":"/publication/graph-2022/","section":"publication","summary":"Graph Neural Networks (GNNs) have shown advantages in various graph-based applications. Most existing GNNs assume strong homophily of graph structure and apply permutation-invariant local aggregation of neighbors to learn a representation for each node. However, they fail to generalize to heterophilic graphs, where most neighboring nodes have different labels or features, and the relevant nodes are distant. Few recent studies attempt to address this problem by combining multiple hops of hidden representations of central nodes (i.e., multi-hop-based approaches) or sorting the neighboring nodes based on attention scores (i.e., rankingbased approaches). As a result, these approaches have some apparent limitations. On the one hand, multi-hop-based approaches do not explicitly distinguish relevant nodes from a large number of multi-hop neighborhoods, leading to a severe over-smoothing problem. On the other hand, ranking-based models do not joint-optimize node ranking with end tasks and result in sub-optimal solutions. In this work, we present Graph Pointer Neural Networks (GPNN) to tackle the challenges mentioned above. We leverage a pointer network to select the most relevant nodes from a large amount of multihop neighborhoods, which constructs an ordered sequence according to the relationship with the central node. 1D convolution is then applied to extract high-level features from the node sequence. The pointer-network-based ranker in GPNN is joint-optimized with other parts in an end-to-end manner. Extensive experiments are conducted on six public node classifcation datasets with heterophilic graphs. The results show that GPNN signifcantly improves the classifcation performance of state-of-the-art methods. In addition, analyses also reveal the privilege of the proposed GPNN in fltering out irrelevant neighbors and reducing over-smoothing.","tags":[],"title":"Graph pointer neural networks","type":"publication"},{"authors":["Zhihan Yue","Yujing Wang","Juanyong Duan","Tianmeng Yang","Congrui Huang","Yunhai Tong","Bixiong Xu"],"categories":[],"content":"","date":1656374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656374400,"objectID":"1234ccbfd962881189b813f23b17b2b6","permalink":"https://MSALab-PKU.github.io/publication/ts2vec-2020/","publishdate":"2022-06-28T00:00:00Z","relpermalink":"/publication/ts2vec-2020/","section":"publication","summary":"This paper presents TS2Vec, a universal framework for learning representations of time series in an arbitrary semantic level. Unlike existing methods, TS2Vec performs contrastive learning in a hierarchical way over augmented context views, which enables a robust contextual representation for each timestamp. Furthermore, to obtain the representation of an arbitrary sub-sequence in the time series, we can apply a simple aggregation over the representations of corresponding timestamps. We conduct extensive experiments on time series classification tasks to evaluate the quality of time series representations. As a result, TS2Vec achieves significant improvement over existing SOTAs of unsupervised time series representation on 125 UCR datasets and 29 UEA datasets. The learned timestamp-level representations also achieve superior results in time series forecasting and anomaly detection tasks. A linear regression trained on top of the learned representations outperforms previous SOTAs of time series forecasting. Furthermore, we present a simple way to apply the learned representations for unsupervised anomaly detection, which establishes SOTA results in the literature. The source code is publicly available at https://github.com/yuezhihan/ts2vec.","tags":[],"title":"Ts2vec: Towards universal representation of time series","type":"publication"},{"authors":["Xiangtai Li","Wenwei Zhang","Jiangmiao Pang","Kai Chen","Guangliang Cheng","Yunhai Tong","Chen Change Loy"],"categories":[],"content":"","date":1648771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648771200,"objectID":"9e90c5b14b42ca769d774c263904f66b","permalink":"https://MSALab-PKU.github.io/publication/video-k-net-2022/","publishdate":"2022-04-01T00:00:00Z","relpermalink":"/publication/video-k-net-2022/","section":"publication","summary":"This paper presents Video K-Net, a simple, strong, and unified framework for fully end-to-end video panoptic segmentation. The method is built upon K-Net, a method that unifies image segmentation via a group of learnable kernels. We observe that these learnable kernels from K-Net, which encode object appearances and contexts, can naturally associate identical instances across video frames. Motivated by this observation, Video K-Net learns to simultaneously segment and track 'things' and 'stuff' in a video with simple kernel-based appearance modeling and cross-temporal kernel interaction. Despite the simplicity, it achieves state-of-the-art video panoptic segmentation results on Citscapes-VPS and KITTI-STEP without bells and whistles. In particular on KITTI-STEP, the simple method can boost almost 12% relative improvements over previous methods. We also validate its generalization on video semantic segmentation, where we boost various baselines by 2% on the VSPW dataset. Moreover, we extend K-Net into clip-level video framework for video instance segmentation where we obtain 40.5% for ResNet50 backbone and 51.5% mAP for Swin-base on YouTube-2019 validation set. We hope this simple yet effective method can serve as a new flexible baseline in video segmentation. Both code and models are released at https://github.com/lxtGH/Video-K-Net.","tags":[],"title":"Video k-net: A simple, strong, and unified baseline for video segmentation","type":"publication"},{"authors":["Hao He","Xiangtai Li","Yibo Yang","Guangliang Cheng","Yunhai Tong","Lubin Weng","Zhouchen Lin","Shiming Xiang"],"categories":[],"content":"","date":1639440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639440000,"objectID":"d505565b6758fd247d528b58ed246576","permalink":"https://MSALab-PKU.github.io/publication/boundarysqueeze-2021/","publishdate":"2021-12-14T00:00:00Z","relpermalink":"/publication/boundarysqueeze-2021/","section":"publication","summary":"This paper proposes a novel method for high-quality image segmentation of both objects and scenes. Inspired by the dilation and erosion operations in morphological image processing techniques, the pixel-level image segmentation problems are treated as squeezing object boundaries. From this perspective, a novel and efficient **Boundary Squeeze** module is proposed. This module is used to squeeze the object boundary from both inner and outer directions, which contributes to precise mask representation. A bi-directionally flow-based warping process is proposed to generate such squeezed feature representation, and two specific loss signals are designed to supervise the squeezing process. The Boundary Squeeze module can be easily applied to both instance and semantic segmentation tasks as a plug-and-play module by building on top of some existing methods. Moreover, the proposed module is lightweighted, and thus has potential for practical usage. Experiment results show that our simple yet effective design can produce high-quality results on several different datasets. Besides, several other metrics on the boundary are used to prove the effectiveness of our method over previous work. Our approach yields significant improvement on challenging COCO and Cityscapes datasets for both instance and semantic segmentation, and outperforms previous state-of-the-art PointRend in both accuracy and speed under the same setting. Codes and models will be published at https://github.com/lxtGH/BSSeg.","tags":[],"title":"BoundarySqueeze: Image Segmentation as Boundary Squeezing","type":"publication"},{"authors":["Yiren Chen","XiaoYu Kou","Jiangang Bai","Yunhai Tong"],"categories":[],"content":"","date":1634860800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634860800,"objectID":"adf249875967c515011a58e2af6d8cfb","permalink":"https://MSALab-PKU.github.io/publication/improving-2021/","publishdate":"2021-10-22T00:00:00Z","relpermalink":"/publication/improving-2021/","section":"publication","summary":"One of the most popular paradigms of applying large pre-trained NLP models such as BERT is to fine-tune it on a smaller dataset. However, one challenge remains as the fine-tuned model often overfits on smaller datasets. A symptom of this phenomenon is that irrelevant or misleading words in the sentence, which are easy to understand for human beings, can substantially degrade the performance of these fine-tuned BERT models. In this paper, we propose a novel technique, called Self-Supervised Attention (SSA) to help facilitate this generalization challenge. Specifically, SSA automatically generates weak, token-level attention labels iteratively by probing the fine-tuned model from the previous iteration. We investigate two different ways of integrating SSA into BERT and propose a hybrid approach to combine their benefits. Empirically, through a variety of public datasets, we illustrate significant performance improvement using our SSA-enhanced BERT model.","tags":[],"title":"Improving BERT with Self-Supervised Attention","type":"publication"},{"authors":["Lu He","Qianyu Zhou","Xiangtai Li","Li Niu","Guangliang Cheng","Xiao Li","Wenxuan Liu","Yunhai Tong","Lizhuang Ma","Liqing Zhang"],"categories":[],"content":"","date":1634428800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634428800,"objectID":"bc6d3d64a1d5a1622cebc4dc4a7e4a0a","permalink":"https://MSALab-PKU.github.io/publication/end-2021/","publishdate":"2021-10-17T00:00:00Z","relpermalink":"/publication/end-2021/","section":"publication","summary":"Recently, DETR and Deformable DETR have been proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance as previous complex hand-crafted detectors. However, their performance on Video Object Detection (VOD) has not been well explored. In this paper, we present TransVOD, an end-to-end video object detection model based on a spatial-temporal Transformer architecture. The goal of this paper is to streamline the pipeline of VOD, effectively removing the need for many hand-crafted components for feature aggregation, e.g., optical flow, recurrent neural networks, relation networks. Besides, benefited from the object query design in DETR, our method does not need complicated post-processing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean. In particular, we present temporal Transformer to aggregate both the spatial object queries and the feature memories of each frame. Our temporal Transformer consists of three components: Temporal Deformable Transformer Encoder (TDTE) to encode the multiple frame spatial details, Temporal Query Encoder (TQE) to fuse object queries, and Temporal Deformable Transformer Decoder to obtain current frame detection results. These designs boost the strong baseline deformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID dataset. TransVOD yields comparable results performance on the benchmark of ImageNet VID. We hope our TransVOD can provide a new perspective for video object detection. Code will be made publicly available at https://github.com/SJTU-LuHe/TransVOD.","tags":[],"title":"End-to-end video object detection with spatial-temporal transformers","type":"publication"},{"authors":["Chen Shi","Xiangtai Li","Yanran Wu","Yunhai Tong","Yi Xu"],"categories":[],"content":"","date":1629676800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629676800,"objectID":"c0436f311a6dcfc0e2880d413be0cd87","permalink":"https://MSALab-PKU.github.io/publication/dynamic-2021/","publishdate":"2021-08-23T00:00:00Z","relpermalink":"/publication/dynamic-2021/","section":"publication","summary":"Representation of semantic context and local details is the essential issue for building modern semantic segmentation models. However, the interrelationship between semantic context and local details is not well explored in previous works. In this paper, we propose a Dynamic Dual Sampling Module (DDSM) to conduct dynamic affinity modeling and propagate semantic context to local details, which yields a more discriminative representation. Specifically, a dynamic sampling strategy is used to sparsely sample representative pixels and channels in the higher layer, forming adaptive compact support for each pixel and channel in the lower layer. The sampled features with high semantics are aggregated according to the affinities and then propagated to detailed lower-layer features, leading to a fine-grained segmentation result with wellpreserved boundaries. Experiment results on both Cityscapes and Camvid datasets validate the effectiveness and efficiency of the proposed approach. Code and models will be available at https://github.com/Fantasticarl/DDSM.","tags":[],"title":"Dynamic Dual Sampling Module For Fine-Grained Semantic Segmentation","type":"publication"},{"authors":["Yanran Wu","Xiangtai Li","Chen Shi","Yunhai Tong","Yang Hua","Tao Song","Ruhui Ma","Haibing Guan"],"categories":[],"content":"","date":1629676800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629676800,"objectID":"6211147f5f504cc13b313bdd268cbe04","permalink":"https://MSALab-PKU.github.io/publication/fast-2021/","publishdate":"2021-08-23T00:00:00Z","relpermalink":"/publication/fast-2021/","section":"publication","summary":"In this paper, we propose an effective method for fast and accurate scene parsing called Bidirectional Alignment Network (BiAlignNet). Previously, one representative work BiSeNet [1] uses two different paths (Context Path and Spatial Path) to achieve balanced learning of semantics and details, respectively. However, the relationship between the two paths is not well explored. We argue that both paths can benefit each other in a complementary way. Motivated by this, we propose a novel network by aligning two-path information into each other through a learned flow field. To avoid the noise and semantic gaps, we introduce a Gated Flow Alignment Module to align both features in a bidirectional way. Moreover, to make the Spatial Path learn more detailed information, we present an edge-guided hard pixel mining loss to supervise the aligned learning process. Our method achieves 80.1% and 78.5% mIoU in validation and test set of Cityscapes while running at 30 FPS with full resolution inputs. Code and models will be available at https://github.com/jojacola/BiAlignNet.","tags":[],"title":"Fast and accurate scene parsing via bi-direction alignment networks","type":"publication"},{"authors":["Xupeng Miao","Nezihe Merve Gürel","Wentao Zhang","Zhichao Han","Bo Li","Wei Min","Susie Xi Rao","Hansheng Ren","Yinan Shan","Yingxia Shao","Yujie Wang","Fan Wu","Hui Xue","Yaming Yang","Zitao Zhang","Yang Zhao","Shuai Zhang","Yujing Wang","Bin Cui","Ce Zhang"],"categories":[],"content":"","date":1628899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628899200,"objectID":"aede640dcc1d04080dd8c7cd0522b842","permalink":"https://MSALab-PKU.github.io/publication/degnn-2021/","publishdate":"2021-08-14T00:00:00Z","relpermalink":"/publication/degnn-2021/","section":"publication","summary":"Mining from graph-structured data is an integral component of graph data management. A recent trending technique, graph convolutional network (GCN), has gained momentum in the graph mining field, and plays an essential part in numerous graph-related tasks. Although the emerging GCN optimization techniques bring improvements to specific scenarios, they perform diversely in different applications and introduce many trial-and-error costs for practitioners. Moreover, existing GCN models often suffer from oversmoothing problem. Besides, the entanglement of various graph patterns could lead to non-robustness and harm the final performance of GCNs. In this work, we propose a simple yet efficient graph decomposition approach to improve the performance of general graph neural networks. We first empirically study existing graph decomposition methods and propose an automatic connectivity-ware graph decomposition algorithm, DeGNN. To provide a theoretical explanation, we then characterize GCN from the information-theoretic perspective and show that under certain conditions, the mutual information between the output after *l* layers and the input of GCN converges to 0 exponentially with respect to *l*. On the other hand, we show that graph decomposition can potentially weaken the condition of such convergence rate, alleviating the information loss when GCN becomes deeper. Extensive experiments on various academic benchmarks and real-world production datasets demonstrate that graph decomposition generally boosts the performance of GNN models. Moreover, our proposed solution DeGNN achieves state-of-the-art performances on almost all these tasks.","tags":[],"title":"DeGNN: Improving Graph Neural Networks with Graph Decomposition","type":"publication"},{"authors":["Xiangtai Li","Li Zhang","Guangliang Cheng","Kuiyuan Yang","Yunhai Tong","Xiatian Zhu","Tao Xiang"],"categories":[],"content":"","date":1627948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627948800,"objectID":"79dbe5f12cd65208f67982fd3339ba81","permalink":"https://MSALab-PKU.github.io/publication/global-2021/","publishdate":"2021-08-03T00:00:00Z","relpermalink":"/publication/global-2021/","section":"publication","summary":"Modelling long-range contextual relationships is critical for pixel-wise prediction tasks such as semantic segmentation. However, convolutional neural networks (CNNs) are inherently limited to model such dependencies due to the naive structure in its building modules (e.g., local convolution kernel). While recent global aggregation methods are beneficial for longrange structure information modelling, they would oversmooth and bring noise to the regions contain fine details (e.g., boundaries and small objects), which are very much cared in the semantic segmentation task. To alleviate this problem, we propose to explore the local context for making the aggregated long-range relationship being distributed more accurately in local regions. In particular, we design a novel local distribution module which models the affinity map between global and local relationship for each pixel adaptively. Integrating existing global aggregation modules, we show that our approach can be modularized as an end-to-end trainable block and easily plugged into existing semantic segmentation networks, giving rise to the GALD networks. Despite its simplicity and versatility, our approach allows us to build new state of the art on major semantic segmentation benchmarks including Cityscapes, ADE20K, Pascal Context, Camvid and COCO-stuff. Code and trained models are released at https://github.com/lxtGH/GALD-DGCNet to foster further research.","tags":[],"title":"Global aggregation then local distribution for scene parsing","type":"publication"},{"authors":["Mingliang Zhang","Fandong Meng","Yunhai Tong","Jie Zhou"],"categories":[],"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"849971c0a474bbc6cbc3aa995eebbe58","permalink":"https://MSALab-PKU.github.io/publication/competence-2021/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/publication/competence-2021/","section":"publication","summary":"Currently, multilingual machine translation is receiving more and more attention since it brings better performance for low resource languages (LRLs) and saves more space. However, existing multilingual machine translation models face a severe challenge: imbalance. As a result, the translation performance of different languages in multilingual translation models are quite different. We argue that this imbalance problem stems from the different learning competencies of different languages. Therefore, we focus on balancing the learning competencies of different languages and propose ***C**ompetence-based **C**urriculum **L**earning for **M**ultilingual Machine Translation*, named CCL-M. Specifically, we firstly define two competencies to help schedule the high resource languages (HRLs) and the low resource languages: 1) *Self-evaluated Competence*, evaluating how well the language itself has been learned; and 2) *HRLs-evaluated Competence*, evaluating whether an LRL is ready to be learned according to *HRLs' Self-evaluated Competence*. Based on the above competencies, we utilize the proposed CCL-M algorithm to gradually add new languages into the training set in a curriculum learning manner. Furthermore, we propose a novel competenceaware dynamic balancing sampling strategy for better selecting training samples in multilingual training. Experimental results show that our approach has achieved a steady and significant performance gain compared to the previous state-of-the-art approach on the TED talks dataset.","tags":[],"title":"Competence-based Curriculum Learning for Multilingual Machine Translation","type":"publication"},{"authors":["Xiangtai Li","Xia Li","Ansheng You","Li Zhang","Guangliang Cheng","Kuiyuan Yang","Yunhai Tong","Zhouchen Lin"],"categories":[],"content":"","date":1627603200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627603200,"objectID":"475b76c28dc0ac0049ac663271fd0a23","permalink":"https://MSALab-PKU.github.io/publication/sfsegnets-2021/","publishdate":"2021-07-30T00:00:00Z","relpermalink":"/publication/sfsegnets-2021/","section":"publication","summary":"Graph-based convolutional model such as non-local block has shown to be effective for strengthening the context modeling ability in convolutional neural networks (CNNs). However, its pixel-wise computational overhead is prohibitive which renders it unsuitable for high resolution imagery. In this paper, we explore the efficiency of context graph reasoning and propose a novel framework called Squeeze Reasoning. Instead of propagating information on the spatial map, we first learn to squeeze the input feature into a channel-wise global vector and perform reasoning within the single vector where the computation cost can be significantly reduced. Specifically, we build the node graph in the vector where each node represents an abstract semantic concept. The refined feature within the same semantic category results to be consistent, which is thus beneficial for downstream tasks. We show that our approach can be modularized as an end-to-end trained block and can be easily plugged into existing networks. Despite its simplicity and being lightweight, the proposed strategy allows us to establish the considerable results on different semantic segmentation datasets and shows significant improvements with respect to strong baselines on various other scene understanding tasks including object detection, instance segmentation and panoptic segmentation. Code is available at https://github.com/lxtGH/SFSegNets.","tags":[],"title":"Towards efficient scene understanding via squeeze reasoning","type":"publication"},{"authors":["Yujing Wang","Yaming Yang","Jiangang Bai","Mingliang Zhang","Jing Bai","Jing Yu","Ce Zhang","Gao Huang","Yunhai Tong"],"categories":[],"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"898fbd6b34f17833bd48730befdaa709","permalink":"https://MSALab-PKU.github.io/publication/evolving-2021/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/publication/evolving-2021/","section":"publication","summary":"Transformer is a ubiquitous model for natural language processing and has attracted wide attentions in computer vision. The attention maps are indispensable for a transformer model to encode the dependencies among input tokens. However, they are learned independently in each layer and sometimes fail to capture precise patterns. In this paper, we propose a novel and generic mechanism based on evolving attention to improve the performance of transformers. On one hand, the attention maps in different layers share common knowledge, thus the ones in preceding layers can instruct the attention in succeeding layers through residual connections. On the other hand, low-level and high-level attentions vary in the level of abstraction, so we adopt convolutional layers to model the evolutionary process of attention maps. The proposed evolving attention mechanism achieves significant performance improvement over various state-of-the-art models for multiple tasks, including image classification, natural language understanding and machine translation.","tags":[],"title":"Evolving attention with residual convolutions","type":"publication"},{"authors":["Jianpeng Chen","Yujing Wang","Ming Zeng","Zongyi Xiang","Bitan Hou","Yunhai Tong","Ole J Mengshoel","Yazhou Ren"],"categories":[],"content":"","date":1624233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624233600,"objectID":"586afc7484eb0f95e3e5743a50341963","permalink":"https://MSALab-PKU.github.io/publication/customizing-2021/","publishdate":"2021-06-21T00:00:00Z","relpermalink":"/publication/customizing-2021/","section":"publication","summary":"Graph Neural Networks (GNNs) have been extensively used for mining graph-structured data with impressive performance. We argue that the paths in a graph imply different semantics for different downstream tasks. However, traditional GNNs do not distinguish among various downstream tasks. To address this problem and learn the high-level semantics for specific task, we design a novel GNN solution, namely Customized Graph Neural Network with Path Reweighting (CustomGNN for short). CustomGNN can automatically learn the high-level semantics for specific downstream tasks, highlight semantically relevant paths, and filter out task-irrelevant information in the graph. In addition, we analyze the semantics learned by CustomGNN and demonstrate its ability to avoid the three inherent problems in traditional GNNs, i.e., over-smoothing, poor robustness, and overfitting. In experiments with the node classification task, CustomGNN achieves state-of-the-art accuracies on 6 out of 7 datasets and competitive accuracy on the rest one.","tags":[],"title":"Customizing Graph Neural Networks using Path Reweighting","type":"publication"},{"authors":["Xiangtai Li","Hao He","Xia Li","Duo Li","Guangliang Cheng","Jianping Shi","Lubin Weng","Yunhai Tong","Zhouchen Lin"],"categories":[],"content":"","date":1624060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624060800,"objectID":"a4738e8454d220bc03a1dbd6d6148d0c","permalink":"https://MSALab-PKU.github.io/publication/pointflow-2021/","publishdate":"2021-06-19T00:00:00Z","relpermalink":"/publication/pointflow-2021/","section":"publication","summary":"Aerial Image Segmentation is a particular semantic segmentation problem and has several challenging characteristics that general semantic segmentation does not have. There are two critical issues: The one is an extremely foreground-background imbalanced distribution, and the other is multiple small objects along with the complex background. Such problems make the recent dense affinity context modeling perform poorly even compared with baselines due to over-introduced background context. To handle these problems, we propose a point-wise affinity propagation module based on the Feature Pyramid Network (FPN) framework, named PointFlow. Rather than dense affinity learning, a sparse affinity map is generated upon selected points between the adjacent features, which reduces the noise introduced by the background while keeping efficiency. In particular, we design a dual point matcher to select points from the salient area and object boundaries, respectively. Experimental results on three different aerial segmentation datasets suggest that the proposed method is more effective and efficient than state-of-the-art general semantic segmentation methods. Especially, our methods achieve the best speed and accuracy trade-off on three aerial benchmarks. Further experiments on three general semantic segmentation datasets prove the generality of our method. Code and models are made available (https://github.com/lxtGH/PFSegNets).","tags":[],"title":"PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation","type":"publication"},{"authors":["Jing Yu","Yuan Chai","Yujing Wang","Yue Hu","Qi Wu"],"categories":[],"content":"","date":1623110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623110400,"objectID":"afc746d40770f550e6f42d7d906788c0","permalink":"https://MSALab-PKU.github.io/publication/cogtree-2021/","publishdate":"2021-06-08T00:00:00Z","relpermalink":"/publication/cogtree-2021/","section":"publication","summary":"Scene graphs are semantic abstraction of images that encourage visual understanding and reasoning. However, the performance of Scene Graph Generation (SGG) is unsatisfactory when faced with biased data in real-world scenarios. Conventional debiasing research mainly studies from the view of balancing data distribution or learning unbiased models and representations, ignoring the correlations among the biased classes. In this work, we analyze this problem from a novel cognition perspective: automatically building a hierarchical cognitive structure from the biased predictions and navigating that hierarchy to locate the relationships, making the tail relationships receive more attention in a coarse-to-fine mode. To this end, we propose a novel debiasing Cognition Tree (CogTree) loss for unbiased SGG. We first build a cognitive structure CogTree to organize the relationships based on the prediction of a biased SGG model. The CogTree distinguishes remarkably different relationships at first and then focuses on a small portion of easily confused ones. Then, we propose a debiasing loss specially for this cognitive structure, which supports coarse-to-fine distinction for the correct relationships. The loss is model-agnostic and consistently boosting the performance of several state-of-the-art models. The code is available at: https://github.com/CYVincent/SceneGraph-Transformer-CogTree.","tags":[],"title":"CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation","type":"publication"},{"authors":["Hao He","Xiangtai Li","Guangliang Cheng","Jianping Shi","Yunhai Tong","Gaofeng Meng","Véronique Prinet","LuBin Weng"],"categories":[],"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"29d921d9ebe34fb820fee044e06c137b","permalink":"https://MSALab-PKU.github.io/publication/enhanced-2021/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/publication/enhanced-2021/","section":"publication","summary":"Glass-like objects such as windows, bottles, and mirrors exist widely in the real world. Sensing these objects has many applications, including robot navigation and grasping. However, this task is very challenging due to the arbitrary scenes behind glass-like objects. This paper aims to solve the glass-like object segmentation problem via enhanced boundary learning. In particular, we first propose a novel refined differential module that outputs finer boundary cues. We then introduce an edge-aware point-based graph convolution network module to model the global shape along the boundary. We use these two modules to design a decoder that generates accurate and clean segmentation results, especially on the object contours. Both modules are lightweight and effective: they can be embedded into various segmentation models. In extensive experiments on three recent glass-like object segmentation datasets, including Trans10k, MSD, and GDD, our approach establishes new state-of-the-art results. We also illustrate the strong generalization properties of our method on three generic segmentation datasets, including Cityscapes, BDD, and COCO Stuff. Code and models will be available for further research.","tags":[],"title":"Enhanced boundary learning for glass-like object segmentation","type":"publication"},{"authors":["Yanran Wu","Xiangtai Li","Chen Shi","Yunhai Tong","Yang Hua","Tao Song","Ruhui Ma","Haibing Guan"],"categories":[],"content":"","date":1621900800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621900800,"objectID":"d81cc8e250d541c9bd0f26368fbff116","permalink":"https://MSALab-PKU.github.io/publication/bialignnet-2021/","publishdate":"2021-05-25T00:00:00Z","relpermalink":"/publication/bialignnet-2021/","section":"publication","summary":"In this paper, we propose an effective method for fast and accurate scene parsing called Bidirectional Alignment Network (BiAlignNet). Previously, one representative work BiSeNet [1] uses two different paths (Context Path and Spatial Path) to achieve balanced learning of semantics and details, respectively. However, the relationship between the two paths is not well explored. We argue that both paths can benefit each other in a complementary way. Motivated by this, we propose a novel network by aligning two-path information into each other through a learned flow field. To avoid the noise and semantic gaps, we introduce a Gated Flow Alignment Module to align both features in a bidirectional way. Moreover, to make the Spatial Path learn more detailed information, we present an edge-guided hard pixel mining loss to supervise the aligned learning process. Our network achieves 80.1parcent and 78.5parcent mIoU in validation and test set of Cityscapes while running at 30 FPS with full resolution inputs. Code and models will be available at https://github.com/jojacola/BiAlignNet.","tags":[],"title":"Fast and Accurate Scene Parsing via Bi-Direction Alignment Networks","type":"publication"},{"authors":["Jiangang Bai","Yujing Wang","Yiren Chen","Yaming Yang","Jing Bai","Jing Yu","Yunhai Tong"],"categories":[],"content":"","date":1615075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615075200,"objectID":"e485792bbe91daf6e1e1bc6a8b47d9de","permalink":"https://MSALab-PKU.github.io/publication/syntax-bert-2021/","publishdate":"2021-03-07T00:00:00Z","relpermalink":"/publication/syntax-bert-2021/","section":"publication","summary":"Pre-trained language models like BERT achieve superior performances in various NLP tasks without explicit consideration of syntactic information. Meanwhile, syntactic information has been proved to be crucial for the success of NLP applications. However, how to incorporate the syntax trees effectively and efficiently into pre-trained Transformers is still unsettled. In this paper, we address this problem by proposing a novel framework named Syntax-BERT. This framework works in a plug-and-play mode and is applicable to an arbitrary pre-trained checkpoint based on Transformer architecture. Experiments on various datasets of natural language understanding verify the effectiveness of syntax trees and achieve consistent improvement over multiple pre-trained models, including BERT, RoBERTa, and T5. At the same time, we also made our experiment code public.","tags":[],"title":"Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees","type":"publication"},{"authors":["Hang Zhao","Yujing Wang","Juanyong Duan","Congrui Huang","Defu Cao","Yunhai Tong","Bixiong Xu","Jing Bai","Jie Tong","Qi Zhang"],"categories":[],"content":"","date":1605571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605571200,"objectID":"307cb7cd9433eb2cacc6e2b8c6c82794","permalink":"https://MSALab-PKU.github.io/publication/multivariate-2020/","publishdate":"2020-11-17T00:00:00Z","relpermalink":"/publication/multivariate-2020/","section":"publication","summary":"Anomaly detection on multivariate time-series is of great importance in both data mining research and industrial applications. Recent approaches have achieved significant progress in this topic, but there is remaining limitations. One major limitation is that they do not capture the relationships between different time-series explicitly, resulting in inevitable false alarms. In this paper, we propose a novel self-supervised framework for multivariate time-series anomaly detection to address this issue. Our framework considers each univariate time-series as an individual feature and includes two graph attention layers in parallel to learn the complex dependencies of multivariate time-series in both temporal and feature dimensions. In addition, our approach jointly optimizes a forecasting-based model and a reconstruction-based model, obtaining better time-series representations through a combination of single-timestamp prediction and reconstruction of the entire time-series. We demonstrate the efficacy of our model through extensive experiments. The proposed method outperforms other state-of-the-art models on three real-world datasets. Further analysis shows that our method has good interpretability and is useful for anomaly diagnosis.","tags":[],"title":"Multivariate time-series anomaly detection via graph attention network","type":"publication"},{"authors":["Yiren Chen","Yaming Yang","Hong Sun","Yujing Wang","Yu Xu","Wei Shen","Rong Zhou","Yunhai Tong","Jing Bai","Ruofei Zhang"],"categories":[],"content":"","date":1603065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603065600,"objectID":"1219724f03a96e15fc82b3a53fa79e41","permalink":"https://MSALab-PKU.github.io/publication/autoadr-2020/","publishdate":"2020-10-19T00:00:00Z","relpermalink":"/publication/autoadr-2020/","section":"publication","summary":"Large-scale pre-trained models have attracted extensive attention in the research community and shown promising results on various tasks of natural language processing. However, these pre-trained models are memory and computation intensive, hindering their deployment into industrial online systems like Ad Relevance. Meanwhile, how to design an effective yet efficient model architecture is another challenging problem in online Ad Relevance. Recently, AutoML shed new lights on architecture design, but how to integrate it with pre-trained language models remains unsettled. In this paper, we propose AutoADR (Automatic model design for AD Relevance) — a novel end-to-end framework to address this challenge, and share our experience to ship these cutting-edge techniques into online Ad Relevance system at Microsoft Bing. Specifically, AutoADR leverages a one-shot neural architecture search algorithm to find a tailored network architecture for Ad Relevance. The search process is simultaneously guided by knowledge distillation from a large pre-trained teacher model (e.g. BERT), while taking the online serving constraints (e.g. memory and latency) into consideration. We add the model designed by AutoADR as a sub-model into the production Ad Relevance model. This additional sub-model improves the Precision-Recall AUC (PR AUC) on top of the original Ad Relevance model by 2.65X of the normalized shipping bar. More importantly, adding this automatically designed sub-model leads to a statistically significant 4.6% Bad-Ad ratio reduction in online A/B testing. This model has been shipped into Microsoft Bing Ad Relevance Production model.","tags":[],"title":"AutoADR: Automatic model design for ad relevance","type":"publication"},{"authors":["Defu Cao","Yujing Wang","Juanyong Duan","Ce Zhang","Xia Zhu","Congrui Huang","Yunhai Tong","Bixiong Xu","Jing Bai","Jie Tong","Qi Zhang"],"categories":[],"content":"","date":1599955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599955200,"objectID":"9cadb24c69ac00bcb8b4ec47bbcf1c5e","permalink":"https://MSALab-PKU.github.io/publication/spectral-2020/","publishdate":"2020-09-13T00:00:00Z","relpermalink":"/publication/spectral-2020/","section":"publication","summary":"Multivariate time-series forecasting plays a crucial role in many real-world applications. It is a challenging problem as one needs to consider both intra-series temporal correlations and inter-series correlations simultaneously. Recently, there have been multiple works trying to capture both correlations, but most, if not all of them only capture temporal correlations in the time domain and resort to pre-defined priors as inter-series relationships.","tags":[],"title":"Spectral temporal graph neural network for multivariate time-series forecasting","type":"publication"},{"authors":["Yueran Bai","Yingying Wang","Yunhai Tong","Yang Yang","Qiyue Liu","Junhui Liu"],"categories":[],"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"e04ebe93560c45ab300ecb086ca9e5ab","permalink":"https://MSALab-PKU.github.io/publication/boundary-2020/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/boundary-2020/","section":"publication","summary":"Temporal action proposal generation plays an important role in video action understanding, which requires localizing high-quality action content precisely. However, generating temporal proposals with both precise boundaries and high-quality action content is extremely challenging. To address this issue, we propose a novel Boundary Content Graph Neural Network (BC-GNN) to model the insightful relations between the boundary and action content of temporal proposals by the graph neural networks. In BC-GNN, the boundaries and content of temporal proposals are taken as the nodes and edges of the graph neural network, respectively, where they are spontaneously linked. Then a novel graph computation operation is proposed to update features of edges and nodes. After that, one updated edge and two nodes it connects are used to predict boundary probabilities and content confidence score, which will be combined to generate a final high-quality proposal. Experiments are conducted on two mainstream datasets: ActivityNet-1.3 and THUMOS14. Without the bells and whistles, BC-GNN outperforms previous state-ofthe-art methods in both temporal action proposal and temporal action detection tasks.","tags":[],"title":"Boundary content graph neural network for temporal action proposal generation","type":"publication"},{"authors":["Xiangtai Li","Xia Li","Li Zhang","Guangliang Cheng","Jianping Shi","Zhouchen Lin","Shaohua Tan","Yunhai Tong"],"categories":[],"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"6bbbcf11e2e33e0a829880b1575ce2db","permalink":"https://MSALab-PKU.github.io/publication/improving-2020/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/improving-2020/","section":"publication","summary":"Existing semantic segmentation approaches either aim to improve the object’s inner consistency by modeling the global context, or refine objects detail along their boundaries by multi-scale feature fusion. In this paper, a new paradigm for semantic segmentation is proposed. Our insight is that appealing performance of semantic segmentation requires explicitly modeling the object body and edge, which correspond to the high and low frequency of the image. To do so, we first warp the image feature by learning a flow field to make the object part more consistent. The resulting body feature and the residual edge feature are further optimized under decoupled supervision by explicitly sampling different parts (body or edge) pixels. We show that the proposed framework with various baselines or backbone networks leads to better object inner consistency and object boundaries. Extensive experiments on four major road scene semantic segmentation benchmarks including Cityscapes, CamVid , KIITI and BDD show that our proposed approach establishes new state of the art while retaining high efficiency in inference. In particular, we achieve 83.7 mIoU % on Cityscape with only fine-annotated data. Code and models are made available to foster any further research (https://github.com/lxtGH/DecoupleSegNets).","tags":[],"title":"Improving semantic segmentation via decoupled body and edge supervision","type":"publication"},{"authors":["Xiangtai Li","Ansheng You","Zhen Zhu","Houlong Zhao","Maoke Yang","Kuiyuan Yang","Shaohua Tan","Yunhai Tong"],"categories":[],"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"12fdec90910559558a69d27df741c810","permalink":"https://MSALab-PKU.github.io/publication/sfsegnets-2020/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/sfsegnets-2020/","section":"publication","summary":"In this paper, we focus on designing effective method for fast and accurate scene parsing. A common practice to improve the performance is to attain high resolution feature maps with strong semantic representation. Two strategies are widely used—atrous convolutions and feature pyramid fusion, are either computation intensive or ineffective. Inspired by the Optical Flow for motion alignment between adjacent video frames, we propose a Flow Alignment Module (FAM) to learn Semantic Flow between feature maps of adjacent levels, and broadcast highlevel features to high resolution features effectively and efficiently. Furthermore, integrating our module to a common feature pyramid structure exhibits superior performance over other real-time methods even on lightweight backbone networks, such as ResNet-18. Extensive experiments are conducted on several challenging datasets, including Cityscapes, PASCAL Context, ADE20K and CamVid. Especially, our network is the first to achieve 80.4% mIoU on Cityscapes with a frame rate of 26 FPS. The code is available at https://github.com/lxtGH/SFSegNets.","tags":[],"title":"Semantic flow for fast and accurate scene parsing","type":"publication"},{"authors":["Yihuan Mao","Yujing Wang","Chufan Wu","Chen Zhang","Yang Wang","Yaming Yang","Quanlu Zhang","Yunhai Tong","Jing Bai"],"categories":[],"content":"","date":1586304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586304000,"objectID":"4890d7a8b150a2c638eb5e2983dc761e","permalink":"https://MSALab-PKU.github.io/publication/ladabert-2020/","publishdate":"2020-04-08T00:00:00Z","relpermalink":"/publication/ladabert-2020/","section":"publication","summary":"BERT is a cutting-edge language representation model pre-trained by a large corpus, which achieves superior performances on various natural language understanding tasks. However, a major blocking issue of applying BERT to online services is that it is memory-intensive and leads to unsatisfactory latency of user requests, raising the necessity of model compression. Existing solutions leverage the knowledge distillation framework to learn a smaller model that imitates the behaviors of BERT. However, the training procedure of knowledge distillation is expensive itself as it requires sufficient training data to imitate the teacher model. In this paper, we address this issue by proposing a hybrid solution named LadaBERT (Lightweight adaptation of BERT through hybrid model compression), which combines the advantages of different model compression methods, including weight pruning, matrix factorization and knowledge distillation. LadaBERT achieves state-of-the-art accuracy on various public datasets while the training overheads can be reduced by an order of magnitude.","tags":[],"title":"Ladabert: Lightweight adaptation of bert through hybrid model compression","type":"publication"},{"authors":["Xiangtai Li","Houlong Zhao","Lei Han","Yunhai Tong","Shaohua Tan","Kuiyuan Yang"],"categories":[],"content":"","date":1585872000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585872000,"objectID":"e2c26d252f52b737235b4eaf56e7ffa7","permalink":"https://MSALab-PKU.github.io/publication/gated-2020/","publishdate":"2020-04-03T00:00:00Z","relpermalink":"/publication/gated-2020/","section":"publication","summary":"Semantic segmentation generates comprehensive understanding of scenes through densely predicting the category for each pixel. High-level features from Deep Convolutional Neural Networks already demonstrate their effectiveness in semantic segmentation tasks, however the coarse resolution of high-level features often leads to inferior results for small/thin objects where detailed information is important. It is natural to consider importing low level features to compensate for the lost detailed information in high-level features. Unfortunately, simply combining multi-level features suffers from the semantic gap among them. In this paper, we propose a new architecture, named Gated Fully Fusion (GFF), to selectively fuse features from multiple levels using gates in a fully connected way. Specifically, features at each level are enhanced by higher-level features with stronger semantics and lower-level features with more details, and gates are used to control the propagation of useful information which significantly reduces the noises during fusion. We achieve the state of the art results on four challenging scene parsing datasets including Cityscapes, Pascal Context, COCO-stuff and ADE20K.","tags":[],"title":"Gated fully fusion for semantic segmentation","type":"publication"},{"authors":["Pengyu Zhao","Ansheng You","Yuanxing Zhang","Jiaying Liu","Kaigui Bian","Yunhai Tong"],"categories":[],"content":"","date":1585872000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585872000,"objectID":"0ea7cbccf8037a981afdea8cf672dc20","permalink":"https://MSALab-PKU.github.io/publication/spherical-2020/","publishdate":"2020-04-03T00:00:00Z","relpermalink":"/publication/spherical-2020/","section":"publication","summary":"With the advance of omnidirectional panoramic technology, 360◦ imagery has become increasingly popular in the past few years. To better understand the 360◦ content, many works resort to the 360◦ object detection and various criteria have been proposed to bound the objects and compute the intersection-over-union (IoU) between bounding boxes based on the common equirectangular projection (ERP) or perspective projection (PSP). However, the existing 360◦ criteria are either inaccurate or inefficient for real-world scenarios. In this paper, we introduce a novel spherical criteria for fast and accurate 360◦ object detection, including both spherical bounding boxes and spherical IoU (SphIoU). Based on the spherical criteria, we propose a novel two-stage 360◦ detector, ie, Reprojection R-CNN, by combining the advantages of both ERP and PSP, yielding efficient and accurate 360◦ object detection. To validate the design of spherical criteria and Reprojection R-CNN, we construct two unbiased synthetic datasets for training and evaluation. Experimental results reveal that compared with the existing criteria, the two-stage detector with spherical criteria achieves the best mAP results under the same inference speed, demonstrating that the spherical criteria can be more suitable for 360◦ object detection. Moreover, Reprojection R-CNN outperforms the previous state-of-the-art methods by over 30% on mAP with competitive speed, which confirms the efficiency and accuracy of the design.","tags":[],"title":"Spherical criteria for fast and accurate 360 object detection","type":"publication"},{"authors":["Yujing Wang","Yaming Yang","Yiren Chen","Jing Bai","Ce Zhang","Guinan Su","Xiaoyu Kou","Yunhai Tong","Mao Yang","Lidong Zhou"],"categories":[],"content":"","date":1585872000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585872000,"objectID":"dbbff262a1335241c5ab9b500c81f689","permalink":"https://MSALab-PKU.github.io/publication/textnas-2020/","publishdate":"2020-04-03T00:00:00Z","relpermalink":"/publication/textnas-2020/","section":"publication","summary":"Learning text representation is crucial for text classification and other language related tasks. There are a diverse set of text representation networks in the literature, and how to find the optimal one is a non-trivial problem. Recently, the emerging Neural Architecture Search (NAS) techniques have demonstrated good potential to solve the problem. Nevertheless, most of the existing works of NAS focus on the search algorithms and pay little attention to the search space. In this paper, we argue that the search space is also an important human prior to the success of NAS in different applications. Thus, we propose a novel search space tailored for text representation. Through automatic search, the discovered network architecture outperforms state-of-the-art models on various public datasets on text classification and natural language inference tasks. Furthermore, some of the design principles found in the automatic network agree well with human intuition.","tags":[],"title":"Textnas: A neural architecture search space tailored for text representation","type":"publication"},{"authors":["Bitan Hou","Yujing Wang","Ming Zeng","Shan Jiang","Ole J Mengshoel","Yunhai Tong","Jing Bai"],"categories":[],"content":"","date":1574294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574294400,"objectID":"37bb85f550105182c44e1e10ea885b4e","permalink":"https://MSALab-PKU.github.io/publication/customized-2019/","publishdate":"2019-11-21T00:00:00Z","relpermalink":"/publication/customized-2019/","section":"publication","summary":"Graph is a natural representation of data for a variety of real-word applications, such as knowledge graph mining, social network analysis and biological network comparison. For these applications, graph embedding is crucial as it provides vector representations of the graph. One limitation of existing graph embedding methods is that their embedding optimization procedures are disconnected from the target application. In this paper, we propose a novel approach, namely Customized Graph Embedding (CGE) to tackle this problem. The CGE algorithm learns customized vector representations of graph nodes by differentiating the importance of distinct graph paths automatically for a specific application. Extensive experiments were carried out on a diverse set of node classification datasets, which demonstrate strong performances of CGE and provide deep insights into the model.","tags":[],"title":"Customized graph embedding: tailoring embedding vectors to different applications","type":"publication"},{"authors":["Xiangtai Li","Li Zhang","Ansheng You","Maoke Yang","Kuiyuan Yang","Yunhai Tong"],"categories":[],"content":"","date":1568592000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568592000,"objectID":"aef0fbd2647252998dad09ef041d45be","permalink":"https://MSALab-PKU.github.io/publication/global-2019/","publishdate":"2019-09-16T00:00:00Z","relpermalink":"/publication/global-2019/","section":"publication","summary":"It has been widely proven that modelling long-range dependencies in fully convolutional networks (FCNs) via global aggregation modules is critical for complex scene understanding tasks such as semantic segmentation and object detection. However, global aggregation is often dominated by features of large patterns and tends to oversmooth regions that contain small patterns (e.g., boundaries and small objects). To resolve this problem, we propose to first use Global Aggregation and then Local Distribution, which is called GALD, where long-range dependencies are more confidently used inside large pattern regions and vice versa. The size of each pattern at each position is estimated in the network as a per-channel mask map. GALD is end-to-end trainable and can be easily plugged into existing FCNs with various global aggregation modules for a wide range of vision tasks, and consistently improves the performance of state-of-the-art object detection and instance segmentation approaches. In particular, GALD used in semantic segmentation achieves new state-of-the-art performance on Cityscapes test set with mIoU 83.3 %. Code is available at: https://github.com/lxtGH/GALD-Net","tags":[],"title":"Global aggregation then local distribution in fully convolutional networks","type":"publication"},{"authors":["Li Zhang","Xiangtai Li","Anurag Arnab","Kuiyuan Yang","Yunhai Tong","Philip HS Torr"],"categories":[],"content":"","date":1568332800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568332800,"objectID":"eae06e9e82b3e1074f22ab0b7d4c66d4","permalink":"https://MSALab-PKU.github.io/publication/dual-2019/","publishdate":"2019-09-13T00:00:00Z","relpermalink":"/publication/dual-2019/","section":"publication","summary":"Exploiting long-range contextual information is key for pixel-wise prediction tasks such as semantic segmentation. In contrast to previous work that uses multi-scale feature fusion or dilated convolutions, we propose a novel graph-convolutional network (GCN) to address this problem. Our Dual Graph Convolutional Network (DGCNet) models the global context of the input feature by modelling two orthogonal graphs in a single framework. The first component models spatial relationships between pixels in the image, whilst the second models interdependencies along the channel dimensions of the network's feature map. This is done efficiently by projecting the feature into a new, lower-dimensional space where all pairwise interactions can be modelled, before reprojecting into the original space. Our simple method provides substantial benefits over a strong baseline and achieves state-of-the-art results on both Cityscapes (82.0% mean IoU) and Pascal Context (53.7% mean IoU) datasets. Code and models are made available to foster any further research (https://github.com/lxtGH/GALD-DGCNet).","tags":[],"title":"Dual graph convolutional network for semantic segmentation","type":"publication"},{"authors":["Xiangtai Li","Jiangang Bai","Kuiyuan Yang","Yunhai Tong"],"categories":[],"content":"","date":1567987200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567987200,"objectID":"927eb17602ca63b4104c2800907af439","permalink":"https://MSALab-PKU.github.io/publication/flow2seg-2019/","publishdate":"2019-09-09T00:00:00Z","relpermalink":"/publication/flow2seg-2019/","section":"publication","summary":"Motion is an important clue for segmentation. In this paper, we leverage motion information densely represented by optical flow to assist the semantic segmentation task. Specifically, our framework takes both image and optical flow as input, where image goes through a state-of-the-art deep network and optical flow goes through a relatively shallow network, and results from both paths are fused together in a residual manner. Unlike image, optical flow is weakly related to semantics but can separate different objects according motion consistency, which motivates us to use relatively shallow network to process optical flow to avoid overfitting and keep spatial information. In our experiment on Cityscapes, we find that optical flow improves image-based segmentation on object boundaries especially on small thin objects. Aided by motion, we achieve comparable results with state-of-the-art methods.","tags":[],"title":"Flow2seg: Motion-aided semantic segmentation","type":"publication"},{"authors":["Pengyu Zhao","Ansheng You","Yuanxing Zhang","Jiaying Liu","Kaigui Bian","Yunhai Tong"],"categories":[],"content":"","date":1564185600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564185600,"objectID":"aabb843592668570fd2146cc71d5d742","permalink":"https://MSALab-PKU.github.io/publication/reprojection-2019/","publishdate":"2019-07-27T00:00:00Z","relpermalink":"/publication/reprojection-2019/","section":"publication","summary":"360 deg images are usually represented in either equirectangular projection (ERP) or multiple perspective projections. Different from the flat 2D images, the detection task is challenging for 360 deg images due to the distortion of ERP and the inefficiency of perspective projections. However, existing methods mostly focus on one of the above representations instead of both, leading to limited detection performance. Moreover, the lack of appropriate bounding-box annotations as well as the annotated datasets further increases the difficulties of the detection task. In this paper, we present a standard object detection framework for 360 deg images. Specifically, we adapt the terminologies of the traditional object detection task to the omnidirectional scenarios, and propose a novel two-stage object detector, i.e., Reprojection R-CNN by combining both ERP and perspective projection. Owing to the omnidirectional field-of-view of ERP, Reprojection R-CNN first generates coarse region proposals efficiently by a distortion-aware spherical region proposal network. Then, it leverages the distortion-free perspective projection and refines the proposed regions by a novel reprojection network. We construct two novel synthetic datasets for training and evaluation. Experiments reveal that Reprojection R-CNN outperforms the previous state-of-the-art methods on the mAP metric. In addition, the proposed detector could run at 178ms per image in the panoramic datasets, which implies its practicability in real-world applications.","tags":[],"title":"Reprojection R-CNN: A Fast and Accurate Object Detector for 360 deg Images","type":"publication"},{"authors":["Xihan Li","Jia Zhang","Jiang Bian","Yunhai Tong","Tie-Yan Liu"],"categories":[],"content":"","date":1551484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551484800,"objectID":"d0939e4bf74ee89e218ed15e3275d439","permalink":"https://MSALab-PKU.github.io/publication/a-2019/","publishdate":"2019-03-02T00:00:00Z","relpermalink":"/publication/a-2019/","section":"publication","summary":"Resource balancing within complex transportation networks is one of the most important problems in real logistics domain. Traditional solutions on these problems leverage combinatorial optimization with demand and supply forecasting. However, the high complexity of transportation routes, severe uncertainty of future demand and supply, together with non-convex business constraints make it extremely challenging in the traditional resource management field. In this paper, we propose a novel sophisticated multi-agent reinforcement learning approach to address these challenges. In particular, inspired by the externalities especially the interactions among resource agents, we introduce an innovative cooperative mechanism for state and reward design resulting in more effective and efficient transportation. Extensive experiments on a simulated ocean transportation service demonstrate that our new approach can stimulate cooperation among agents and lead to much better performance. Compared with traditional solutions based on combinatorial optimization, our approach can give rise to a significant improvement in terms of both performance and stability.","tags":[],"title":"A cooperative multi-agent reinforcement learning framework for resource balancing in complex logistics network","type":"publication"},{"authors":["Yingxia Shao","Kai Lei","Lei Chen","Zi Huang","Bin Cui","Zhongyi Liu","Yunhai Tong","Jin Xu"],"categories":[],"content":"","date":1497830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497830400,"objectID":"d24c744e5660051b8417bd3b4e29f302","permalink":"https://MSALab-PKU.github.io/publication/fast-2017/","publishdate":"2017-06-19T00:00:00Z","relpermalink":"/publication/fast-2017/","section":"publication","summary":"Heterogeneous graph is a popular data model to represent the real-world relations with abundant semantics. To analyze heterogeneous graphs, an important step is extracting homogeneous graphs from the heterogeneous graphs, called homogeneous graph extraction. In an extracted homogeneous graph, the relation is defined by a line pattern on the heterogeneous graph and the new attribute values of the relation are calculated by user-defined aggregate functions. The key challenges of the extraction problem are how to efficiently enumerate paths matched by the line pattern and aggregate values for each pair of vertices from the matched paths. To address above two challenges, we propose a parallel graph extraction framework, where we use vertex-centric model to enumerate paths and compute aggregate functions in parallel. The framework compiles the line pattern into a path concatenation plan, which determines the order of concatenating paths and generates the final paths in a divide-and-conquer manner. We introduce a cost model to estimate the cost of a plan and discuss three plan selection strategies, among which the best plan can enumerate paths in OðlogðlÞÞ iterations, where l is the length of a pattern. Furthermore, to improve the performance of evaluating aggregate functions, we classify the aggregate functions into three categories, i.e., distributive aggregation, algebraic aggregation, and holistic aggregation. Since the distributive and algebraic aggregations can be computed from the partial paths, we speed up the aggregation by computing partial aggregate values during the path enumeration.","tags":[],"title":"Fast parallel path concatenation for graph extraction","type":"publication"},{"authors":["Jiawei Jiang","Yunhai Tong","Hua Lu","Bin Cui","Kai Lei","Lele Yu"],"categories":[],"content":"","date":1496620800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496620800,"objectID":"89e7382d6c063f34ae1c18516018646b","permalink":"https://MSALab-PKU.github.io/publication/gvos-2017/","publishdate":"2017-06-05T00:00:00Z","relpermalink":"/publication/gvos-2017/","section":"publication","summary":"The exponential increase of online videos greatly enriches the life of users but also brings huge numbers of near-duplicate videos (NDVs) that seriously challenge the video websites. The video websites entail NDV-related applications such as detection of copyright violation, video monitoring, video re-ranking, and video recommendation. Since these applications adopt different features and different processing procedures due to diverse scenarios, constructing separate and special-purpose systems for them incurs considerable costs on design, implementation, and maintenance. In this article, we propose a general NDV system on Storm (GVoS)—a popular distributed real-time stream processing platform—to simultaneously support a wide variety of video applications. The generality of GVoS is achieved in two aspects. First, we extract the reusable components from various applications. Second, we conduct the communication between components via a mechanism called Stream Shared Message (SSM) that contains the video-related data. Furthermore, we present an algorithm to reduce the size of SSM in order to avoid the data explosion and decrease the network latency. The experimental results demonstrate that GVoS can achieve performance almost the same as the customized systems. Meanwhile, GVoS accomplishes remarkably higher systematic versatility and efficiently facilitates the development of various NDV-related applications.","tags":[],"title":"Gvos: a general system for near-duplicate video-related applications on storm","type":"publication"},{"authors":["Xiangfeng Meng","Yunhai Tong","Xinhai Liu","Yiren Chen","Shaohua Tan"],"categories":[],"content":"","date":1492128000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492128000,"objectID":"6bc436f58efee489e6e8296a59801ef3","permalink":"https://MSALab-PKU.github.io/publication/a-2017/","publishdate":"2017-04-14T00:00:00Z","relpermalink":"/publication/a-2017/","section":"publication","summary":"Graph similarity has been a crucial topic in network science, and is widely used in network dynamics, graph monitoring and anomalous event detection. However, few studies have paid attention to community similarity. The fact that communities do not necessarily own sub-modularity structure determines that graph similarity algorithms can not be applied to communities directly. Besides, the existing graph similarity algorithms ignore the organization structure of networks. Two communities can be regarded as the same when both their vertices and structure are identical. Thus the existing algorithms are unable to detect anomalous events about the shift of communities’ organization structure. In this paper, we propose a novel community similarity algorithm, which considers both the shift of vertices and the shift of communities’ layered structure. The layered structure of communities categorizes nodes into different groups, depending on their influence in the community. Both the influence of each node and the shift of nodes’ influence are expected to affect the similarity of two communities. Experiments on the synthetic data show that the novel algorithm performs better than the state-of-art algorithms. Besides, we apply the novel algorithm on the scientific data set, and identify meaningful anomalous events occurred in scientific mapping. The anomalous events are proved to correspond to the transition of topics for journal communities. It demonstrates that the novel algorithm is effective in detecting the anomalous events about the transition of communities’ structure.","tags":[],"title":"A Structural Based Community Similarity Algorithm and Its Application in Scientific Event Detection","type":"publication"},{"authors":["Xiangfeng Meng","Yunhai Tong","Xinhai Liu","Yiren Chen","Shaohua Tan"],"categories":[],"content":"","date":1490140800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490140800,"objectID":"22e883ee51e4edf64451fe5515cb7be4","permalink":"https://MSALab-PKU.github.io/publication/netrating-2017/","publishdate":"2017-03-22T00:00:00Z","relpermalink":"/publication/netrating-2017/","section":"publication","summary":"Guaranteed loans are a common way for enterprises to raise money from banks without any collateral in China. The enterprises are highly intertwined with each other, and hence form a densely connected guarantee network. As the economy is down in recent years, the default risk spreads along with the guarantee relations, and has caused great financial risk in many regions of China. Thus it puts forward a new challenge for financial regulators to monitor the enterprises involved in the guarantee network and control the system risk. However, the traditional financial risk management are based on vector space models, and could not handle the relations among enterprises. In this paper, based on the k-shell decomposition method, we propose a novel risk evaluation strategy, NetRating, to assess the risk level of each enterprise involved in the guaranteed loans. Besides, to deal with the direct guarantee networks, we propose the directed k-shell decomposition method, and extend NetRating strategy to the directed NetRating strategy. The application of our strategy in the real data verifies its effectiveness in credit assessment. It indicates that our strategy can provide a novel perspective for financial regulators to monitor the guarantee networks and control potential system risk.","tags":[],"title":"Netrating: Credit risk evaluation for loan guarantee chain in china","type":"publication"},{"authors":["Jiawei Jiang","Zhipeng Zhang","Bin Cui","Yunhai Tong","Ning Xu"],"categories":[],"content":"","date":1490140800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490140800,"objectID":"1628a905686ea79681948e06c71b252d","permalink":"https://MSALab-PKU.github.io/publication/stromax-2017/","publishdate":"2017-03-22T00:00:00Z","relpermalink":"/publication/stromax-2017/","section":"publication","summary":"With the increasing availability and scale of data from Web 2.0, the ability to efficiently and timely analyze huge amounts of data is important for industry success. A number of real-time stream processing platforms have been developed, such as Storm, S4, and Flume. A fundamental problem of these large scale decentralized stream processing systems is how to deploy the workload to each node so as to fully utilize the available resources and optimize the overall system performance. In this paper, we present StroMAX, a graph-partitioning based approach of workload scheduling for real-time stream processing systems. StroMAX uses two advanced generic schedulers to improve the performance of stream processing systems by reducing the inter-node communication cost while keeping the workload of nodes below a certain computational load threshold. The first scheduler analyzes the workload structure when a job is committed and uses the graph-partitioning result to determine the deployment of tasks. The second scheduler analyzes the statistical information of physical nodes, and dynamically reassigns the tasks during runtime to improve the overall performance. Besides, StroMAXcan be deployed to many other state-of-the-art real-time stream processing systems easily. We implemented StroMAX on Storm, a representative real-time stream processing system. Extensive experiments conducted with real-world workloads and datasets demonstrate the superiority of our approaches against the existing solutions.","tags":[],"title":"StroMAX: Partitioning-based scheduler for real-time stream processing system","type":"publication"},{"authors":["Yujing Wang","Yunhai Tong","Ming Zeng"],"categories":[],"content":"","date":1372550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372550400,"objectID":"e56ca65fdbcc6d77e53c532e167605ad","permalink":"https://MSALab-PKU.github.io/publication/ranking-2013/","publishdate":"2013-06-30T00:00:00Z","relpermalink":"/publication/ranking-2013/","section":"publication","summary":"Ranking scientific articles is an important but challenging task, partly due to the dynamic nature of the evolving publication network. In this paper, we mainly focus on two problems:(1) how to rank articles in the heterogeneous network; and (2) how to use time information in the dynamic network in order to obtain a better ranking result. To tackle the problems, we propose a graph based ranking method, which utilizes citations, authors, journals/conferences and the publication time information collaboratively. The experiments were carried out on two public datasets. The result shows that our approach is practical and ranks scientific articles more accurately than the state-of-art methods.","tags":[],"title":"Ranking scientific articles by exploiting citations, authors, journals, and time information","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://MSALab-PKU.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://MSALab-PKU.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]