[{"authors":["chan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"56687b7fcbaceb3c4ed6d5b35f5c4e2a","permalink":"https://MSALab-PKU.github.io/author/chang-han/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chang-han/","section":"authors","summary":"","tags":null,"title":"Chang Han","type":"authors"},{"authors":["csun"],"categories":null,"content":"Biography I am currently a Ph.D. candidate in Intelligent Science and Technology at Peking University, focusing on advanced computational models and data analysis. My academic journey began with a Bachelor’s degree in Information Management and Information Systems from Nanjing Agricultural University, followed by a Master\u0026rsquo;s degree in Library Science from the University of Chinese Academy of Sciences. My research interests lie in the fields of recommender systems, large language models, graph data, digital libraries, and deep reinforcement learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"be26fadc8643576e89752107a6fac3ee","permalink":"https://MSALab-PKU.github.io/author/chao-sun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chao-sun/","section":"authors","summary":"Biography I am currently a Ph.D. candidate in Intelligent Science and Technology at Peking University, focusing on advanced computational models and data analysis. My academic journey began with a Bachelor’s degree in Information Management and Information Systems from Nanjing Agricultural University, followed by a Master\u0026rsquo;s degree in Library Science from the University of Chinese Academy of Sciences.","tags":null,"title":"Chao Sun","type":"authors"},{"authors":["cywang"],"categories":null,"content":"Biography I am currently a PhD Student at School of Intelligence Science and Technology, Peking University (PKU), advised by Prof. Yunhai Tong. My research interest includes computer vision and machine learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a3788e3637336a57e66336b927f9efc8","permalink":"https://MSALab-PKU.github.io/author/chaoyang-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chaoyang-wang/","section":"authors","summary":"Biography I am currently a PhD Student at School of Intelligence Science and Technology, Peking University (PKU), advised by Prof. Yunhai Tong. My research interest includes computer vision and machine learning.","tags":null,"title":"Chaoyang Wang","type":"authors"},{"authors":["jhmeng"],"categories":null,"content":"Biography I am currently pursuing my undergraduate degree at the School of Electronics Engineering and Computer Science, Peking University. Starting in autumn 2024, I will begin a Ph.D. program at the School of Intelligence Science and Technology, Peking University. Under the mentorship of Professor Yunhai Tong, my research primarily focus on computer vision and graph neural networks. I am deeply passionate about exploring innovative solutions within these domains and contributing to the advancement of artificial intelligence.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"adec5cfb7a28c6920b23634f788104e3","permalink":"https://MSALab-PKU.github.io/author/jiahao-meng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiahao-meng/","section":"authors","summary":"Biography I am currently pursuing my undergraduate degree at the School of Electronics Engineering and Computer Science, Peking University. Starting in autumn 2024, I will begin a Ph.D. program at the School of Intelligence Science and Technology, Peking University.","tags":null,"title":"Jiahao Meng","type":"authors"},{"authors":["jgbai"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f3a76360065a3359cd90463bea76f6dc","permalink":"https://MSALab-PKU.github.io/author/jiangang-bai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiangang-bai/","section":"authors","summary":"","tags":null,"title":"Jiangang Bai","type":"authors"},{"authors":["jzwu"],"categories":null,"content":"Biography I am Jianzong Wu (吴健宗) and I am a PhD Student at School of Intelligence Science and Technology, Peking University (PKU), advised by Prof. Yunhai Tong. Previously, I obtained my bachelor’s degree at University of Science and Technology of China (USTC).\nMy research interest focuses on multi-modal learning, including feature alignment, scene understanding and content generation. So far, I have conducted research works on referring image segmentation, open vocabulary image segmentation, text-to-image editting task, multi-modal large language models, as well as several related fields.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5a0c8c939b2c265a97fc98ef25b2275b","permalink":"https://MSALab-PKU.github.io/author/jianzong-wu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jianzong-wu/","section":"authors","summary":"Biography I am Jianzong Wu (吴健宗) and I am a PhD Student at School of Intelligence Science and Technology, Peking University (PKU), advised by Prof. Yunhai Tong. Previously, I obtained my bachelor’s degree at University of Science and Technology of China (USTC).","tags":null,"title":"Jianzong Wu","type":"authors"},{"authors":["jbhu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3ed410aac5b4f052f61a64ee927c14cf","permalink":"https://MSALab-PKU.github.io/author/jingbo-hu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jingbo-hu/","section":"authors","summary":"","tags":null,"title":"Jingbo Hu","type":"authors"},{"authors":["mlzhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"dfedae070636eb70976aabdd2adb501f","permalink":"https://MSALab-PKU.github.io/author/mingliang-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mingliang-zhang/","section":"authors","summary":"","tags":null,"title":"Mingliang Zhang","type":"authors"},{"authors":["qyshi"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0f97a44b7cbe240cbb6052d147ee51c8","permalink":"https://MSALab-PKU.github.io/author/qingyu-shi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/qingyu-shi/","section":"authors","summary":"","tags":null,"title":"Qingyu Shi","type":"authors"},{"authors":["slxu"],"categories":null,"content":"Biography I am a PhD student at Peking University (PKU) , supervised by Professor Yunhai Tong. I also work closely with Dr.Xiangtai Li and accept his supervision. My research interests include computer vision, machine learning, and scene understanding.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"68ff3b5443a4ade8666c671627510859","permalink":"https://MSALab-PKU.github.io/author/shilin-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shilin-xu/","section":"authors","summary":"Biography I am a PhD student at Peking University (PKU) , supervised by Professor Yunhai Tong. I also work closely with Dr.Xiangtai Li and accept his supervision. My research interests include computer vision, machine learning, and scene understanding.","tags":null,"title":"Shilin Xu","type":"authors"},{"authors":["YoungTimmy"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b21181240bd935c7972236f1a9d91a55","permalink":"https://MSALab-PKU.github.io/author/tianmeng-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tianmeng-yang/","section":"authors","summary":"","tags":null,"title":"Tianmeng Yang","type":"authors"},{"authors":["xtli"],"categories":null,"content":"Biography Xiangtai is a research scientist at Tiktok, Singapore. He received the Ph.D. degree from Peking University in 2022. He worked as a Research Fellow in S-Lab, a member of the Multimedia Laboratory of NTU (MMLab@NTU) at Nanyang Technological University in 2023. His research interests include computer vision and machine learning with a focus on scene understanding, segmentation, video understanding and multi-modal learning. Several of his works have been published in top-tier conferences and journals. He serves as a regular reviewer for top-tier conferences and journals, including CVPR, ICCV, ICLR, NeurIPS, T-PAMI, and IJCV.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b15b1dd349fae079cb30852b753352c5","permalink":"https://MSALab-PKU.github.io/author/xiangtai-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiangtai-li/","section":"authors","summary":"Biography Xiangtai is a research scientist at Tiktok, Singapore. He received the Ph.D. degree from Peking University in 2022. He worked as a Research Fellow in S-Lab, a member of the Multimedia Laboratory of NTU (MMLab@NTU) at Nanyang Technological University in 2023.","tags":null,"title":"Xiangtai Li","type":"authors"},{"authors":["ymyang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a8a395d490050dea6e89bf73db842358","permalink":"https://MSALab-PKU.github.io/author/yaming-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yaming-yang/","section":"authors","summary":"","tags":null,"title":"Yaming Yang","type":"authors"},{"authors":["yichen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2c3dc4301bb02fdc04677974cb73959a","permalink":"https://MSALab-PKU.github.io/author/yiren-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yiren-chen/","section":"authors","summary":"","tags":null,"title":"Yiren Chen","type":"authors"},{"authors":["yjwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b675a23079a8b1fab71d06f02d9bf8fe","permalink":"https://MSALab-PKU.github.io/author/yujing-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yujing-wang/","section":"authors","summary":"","tags":null,"title":"Yujing Wang","type":"authors"},{"authors":null,"categories":null,"content":"Biography Prof. Yunhai Tong currently is a Professor with the School of Artificial Intelligence, Peking University. Before that, he received his Ph.D. degree in computer science from Peking University in 2002. His main research interests include media intelligent computing, deep learning, and multimodal learning. Most of his works in computer vision, natural language processing, and machine learning, including PAMI, IJCV, EMNLP, ECCV, and CVPR, have been published in top-tier conferences and journals. He has led several national research and development projects (National Key Research and Development Program of China) since 2018.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fe6f7929557162f2fd6242ea4c7d82ee","permalink":"https://MSALab-PKU.github.io/author/yunhai-tong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yunhai-tong/","section":"authors","summary":"Biography Prof. Yunhai Tong currently is a Professor with the School of Artificial Intelligence, Peking University. Before that, he received his Ph.D. degree in computer science from Peking University in 2002.","tags":null,"title":"Yunhai Tong","type":"authors"},{"authors":["zmiao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"db1f392f869907e89db7137c5bed49a2","permalink":"https://MSALab-PKU.github.io/author/zheng-miao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zheng-miao/","section":"authors","summary":"","tags":null,"title":"Zheng Miao","type":"authors"},{"authors":["zjlin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9e45d01cc28563997db9fc47cedc0a1e","permalink":"https://MSALab-PKU.github.io/author/zhengjie-lin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhengjie-lin/","section":"authors","summary":"","tags":null,"title":"Zhengjie Lin","type":"authors"},{"authors":["Hao He","Xiangtai Li","Yibo Yang","Guangliang Cheng","Yunhai Tong","Lubin Weng","Zhouchen Lin","Shiming Xiang"],"categories":[],"content":"","date":1639440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639440000,"objectID":"d505565b6758fd247d528b58ed246576","permalink":"https://MSALab-PKU.github.io/publication/boundarysqueeze-2021/","publishdate":"2021-12-14T00:00:00Z","relpermalink":"/publication/boundarysqueeze-2021/","section":"publication","summary":"This paper proposes a novel method for high-quality image segmentation of both objects and scenes. Inspired by the dilation and erosion operations in morphological image processing techniques, the pixel-level image segmentation problems are treated as squeezing object boundaries. From this perspective, a novel and efficient **Boundary Squeeze** module is proposed. This module is used to squeeze the object boundary from both inner and outer directions, which contributes to precise mask representation. A bi-directionally flow-based warping process is proposed to generate such squeezed feature representation, and two specific loss signals are designed to supervise the squeezing process. The Boundary Squeeze module can be easily applied to both instance and semantic segmentation tasks as a plug-and-play module by building on top of some existing methods. Moreover, the proposed module is lightweighted, and thus has potential for practical usage. Experiment results show that our simple yet effective design can produce high-quality results on several different datasets. Besides, several other metrics on the boundary are used to prove the effectiveness of our method over previous work. Our approach yields significant improvement on challenging COCO and Cityscapes datasets for both instance and semantic segmentation, and outperforms previous state-of-the-art PointRend in both accuracy and speed under the same setting. Codes and models will be published at https://github.com/lxtGH/BSSeg.","tags":[],"title":"BoundarySqueeze: Image Segmentation as Boundary Squeezing","type":"publication"},{"authors":["Xupeng Miao","Nezihe Merve Gürel","Wentao Zhang","Zhichao Han","Bo Li","Wei Min","Susie Xi Rao","Hansheng Ren","Yinan Shan","Yingxia Shao","Yujie Wang","Fan Wu","Hui Xue","Yaming Yang","Zitao Zhang","Yang Zhao","Shuai Zhang","Yujing Wang","Bin Cui","Ce Zhang"],"categories":[],"content":"","date":1628899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628899200,"objectID":"aede640dcc1d04080dd8c7cd0522b842","permalink":"https://MSALab-PKU.github.io/publication/degnn-2021/","publishdate":"2021-08-14T00:00:00Z","relpermalink":"/publication/degnn-2021/","section":"publication","summary":"Mining from graph-structured data is an integral component of graph data management. A recent trending technique, graph convolutional network (GCN), has gained momentum in the graph mining field, and plays an essential part in numerous graph-related tasks. Although the emerging GCN optimization techniques bring improvements to specific scenarios, they perform diversely in different applications and introduce many trial-and-error costs for practitioners. Moreover, existing GCN models often suffer from oversmoothing problem. Besides, the entanglement of various graph patterns could lead to non-robustness and harm the final performance of GCNs. In this work, we propose a simple yet efficient graph decomposition approach to improve the performance of general graph neural networks. We first empirically study existing graph decomposition methods and propose an automatic connectivity-ware graph decomposition algorithm, DeGNN. To provide a theoretical explanation, we then characterize GCN from the information-theoretic perspective and show that under certain conditions, the mutual information between the output after *l* layers and the input of GCN converges to 0 exponentially with respect to *l*. On the other hand, we show that graph decomposition can potentially weaken the condition of such convergence rate, alleviating the information loss when GCN becomes deeper. Extensive experiments on various academic benchmarks and real-world production datasets demonstrate that graph decomposition generally boosts the performance of GNN models. Moreover, our proposed solution DeGNN achieves state-of-the-art performances on almost all these tasks.","tags":[],"title":"DeGNN: Improving Graph Neural Networks with Graph Decomposition","type":"publication"},{"authors":["Mingliang Zhang","Fandong Meng","Yunhai Tong","Jie Zhou"],"categories":[],"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"65b716bb12fa77f3876e7d5e1fd0c128","permalink":"https://MSALab-PKU.github.io/publication/ccl-m-2021/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/publication/ccl-m-2021/","section":"publication","summary":"Currently, multilingual machine translation is receiving more and more attention since it brings better performance for low resource languages (LRLs) and saves more space. However, existing multilingual machine translation models face a severe challenge: imbalance. As a result, the translation performance of different languages in multilingual translation models are quite different. We argue that this imbalance problem stems from the different learning competencies of different languages. Therefore, we focus on balancing the learning competencies of different languages and propose ***C**ompetence-based **C**urriculum **L**earning for **M**ultilingual Machine Translation*, named CCL-M. Specifically, we firstly define two competencies to help schedule the high resource languages (HRLs) and the low resource languages: 1) *Self-evaluated Competence*, evaluating how well the language itself has been learned; and 2) *HRLs-evaluated Competence*, evaluating whether an LRL is ready to be learned according to *HRLs' Self-evaluated Competence*. Based on the above competencies, we utilize the proposed CCL-M algorithm to gradually add new languages into the training set in a curriculum learning manner. Furthermore, we propose a novel competenceaware dynamic balancing sampling strategy for better selecting training samples in multilingual training. Experimental results show that our approach has achieved a steady and significant performance gain compared to the previous state-of-the-art approach on the TED talks dataset.","tags":[],"title":"Competence-based Curriculum Learning for Multilingual Machine Translation","type":"publication"},{"authors":["Jing Yu","Yuan Chai","Yujing Wang","Yue Hu","Qi Wu"],"categories":[],"content":"","date":1623110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623110400,"objectID":"afc746d40770f550e6f42d7d906788c0","permalink":"https://MSALab-PKU.github.io/publication/cogtree-2021/","publishdate":"2021-06-08T00:00:00Z","relpermalink":"/publication/cogtree-2021/","section":"publication","summary":"Scene graphs are semantic abstraction of images that encourage visual understanding and reasoning. However, the performance of Scene Graph Generation (SGG) is unsatisfactory when faced with biased data in real-world scenarios. Conventional debiasing research mainly studies from the view of balancing data distribution or learning unbiased models and representations, ignoring the correlations among the biased classes. In this work, we analyze this problem from a novel cognition perspective: automatically building a hierarchical cognitive structure from the biased predictions and navigating that hierarchy to locate the relationships, making the tail relationships receive more attention in a coarse-to-fine mode. To this end, we propose a novel debiasing Cognition Tree (CogTree) loss for unbiased SGG. We first build a cognitive structure CogTree to organize the relationships based on the prediction of a biased SGG model. The CogTree distinguishes remarkably different relationships at first and then focuses on a small portion of easily confused ones. Then, we propose a debiasing loss specially for this cognitive structure, which supports coarse-to-fine distinction for the correct relationships. The loss is model-agnostic and consistently boosting the performance of several state-of-the-art models. The code is available at: https://github.com/CYVincent/SceneGraph-Transformer-CogTree.","tags":[],"title":"CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation","type":"publication"},{"authors":["Chen Shi","Xiangtai Li","Yanran Wu","Yunhai Tong","Yi Xu"],"categories":[],"content":"","date":1621900800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621900800,"objectID":"c324d06dd77d0752bfd2d001bc69bd82","permalink":"https://MSALab-PKU.github.io/publication/ddsm-2021/","publishdate":"2021-05-25T00:00:00Z","relpermalink":"/publication/ddsm-2021/","section":"publication","summary":"Representation of semantic context and local details is the essential issue for building modern semantic segmentation models. However, the interrelationship between semantic context and local details is not well explored in previous works. In this paper, we propose a Dynamic Dual Sampling Module (DDSM) to conduct dynamic affinity modeling and propagate semantic context to local details, which yields a more discriminative representation. Specifically, a dynamic sampling strategy is used to sparsely sample representative pixels and channels in the higher layer, forming adaptive compact support for each pixel and channel in the lower layer. The sampled features with high semantics are aggregated according to the affinities and then propagated to detailed lower-layer features, leading to a fine-grained segmentation result with wellpreserved boundaries. Experiment results on both Cityscapes and Camvid datasets validate the effectiveness and efficiency of the proposed approach. Code and models will be available at https://github.com/Fantasticarl/DDSM.","tags":[],"title":"Dynamic Dual Sampling Module For Fine-Grained Semantic Segmentation","type":"publication"},{"authors":["Yanran Wu","Xiangtai Li","Chen Shi","Yunhai Tong","Yang Hua","Tao Song","Ruhui Ma","Haibing Guan"],"categories":[],"content":"","date":1621900800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621900800,"objectID":"d81cc8e250d541c9bd0f26368fbff116","permalink":"https://MSALab-PKU.github.io/publication/bialignnet-2021/","publishdate":"2021-05-25T00:00:00Z","relpermalink":"/publication/bialignnet-2021/","section":"publication","summary":"In this paper, we propose an effective method for fast and accurate scene parsing called Bidirectional Alignment Network (BiAlignNet). Previously, one representative work BiSeNet [1] uses two different paths (Context Path and Spatial Path) to achieve balanced learning of semantics and details, respectively. However, the relationship between the two paths is not well explored. We argue that both paths can benefit each other in a complementary way. Motivated by this, we propose a novel network by aligning two-path information into each other through a learned flow field. To avoid the noise and semantic gaps, we introduce a Gated Flow Alignment Module to align both features in a bidirectional way. Moreover, to make the Spatial Path learn more detailed information, we present an edge-guided hard pixel mining loss to supervise the aligned learning process. Our network achieves 80.1parcent and 78.5parcent mIoU in validation and test set of Cityscapes while running at 30 FPS with full resolution inputs. Code and models will be available at https://github.com/jojacola/BiAlignNet.","tags":[],"title":"Fast and Accurate Scene Parsing via Bi-Direction Alignment Networks","type":"publication"},{"authors":["Jianzong Wu","Xiangtai Li","Chenyang Si","Shangchen Zhou","Jingkang Yang","Jiangning Zhang","Yining Li","Kai Chen","Yunhai Tong","Ziwei Liu","Chen Change Loy"],"categories":[],"content":"","date":2024,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":2024,"objectID":"5b8dc36e3dce63bd259d3a8f3ead5f9f","permalink":"https://MSALab-PKU.github.io/publication/language-driven-2024/","publishdate":"1970-01-01T08:33:44+08:00","relpermalink":"/publication/language-driven-2024/","section":"publication","summary":"We introduce a new task -- language-driven video inpainting, which uses natural language instructions to guide the inpainting process. This approach overcomes the limitations of traditional video inpainting methods that depend on manually labeled binary masks, a process often tedious and labor-intensive. We present the Remove Objects from Videos by Instructions (ROVI) dataset, containing 5,650 videos and 9,091 inpainting results, to support training and evaluation for this task. We also propose a novel diffusion-based language-driven video inpainting framework, the first end-to-end baseline for this task, integrating Multimodal Large Language Models to understand and execute complex language-based inpainting requests effectively. Our comprehensive results showcase the dataset's versatility and the model's effectiveness in various language-instructed inpainting scenarios. We will make datasets, code, and models publicly available.","tags":[],"title":"Towards language-driven video inpainting via multimodal large language models","type":"publication"},{"authors":["Jianzong Wu","Xiangtai Li","Henghui Ding","Xia Li","Guangliang Cheng","Yunhai Tong","Chen Change Loy"],"categories":[],"content":"","date":2023,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":2023,"objectID":"565db746f149f9fe56355788471cd56a","permalink":"https://MSALab-PKU.github.io/publication/betrayed-2023/","publishdate":"1970-01-01T08:33:43+08:00","relpermalink":"/publication/betrayed-2023/","section":"publication","summary":"In this work, we focus on open vocabulary instance segmentation to expand a segmentation model to classify and segment instance-level novel categories. Previous approaches have relied on massive caption datasets and complex pipelines to establish one-to-one mappings between image regions and words in captions. However, such methods build noisy supervision by matching non-visible words to image regions, such as adjectives and verbs. Meanwhile, context words are also important for inferring the existence of novel objects as they show high inter-correlations with novel categories. To overcome these limitations, we devise a joint Caption Grounding and Generation (CGG) framework, which incorporates a novel grounding loss that only focuses on matching object nouns to improve learning efficiency. We also introduce a caption generation head that enables additional supervision and contextual modeling as a complementation to the grounding loss. Our analysis and results demonstrate that grounding and generation components complement each other, significantly enhancing the segmentation performance for novel classes. Experiments on the COCO dataset with two settings: Open Vocabulary Instance Segmentation (OVIS) and Open Set Panoptic Segmentation (OSPS) demonstrate the superiority of the CGG. Specifically, CGG achieves a substantial improvement of 6.8% mAP for novel classes without extra data on the OVIS task and 15% PQ improvements for novel classes on the OSPS benchmark.","tags":[],"title":"Betrayed by captions: Joint caption grounding and generation for open vocabulary instance segmentation","type":"publication"},{"authors":["Hao He","Xiangtai Li","Guangliang Cheng","Jianping Shi","Yunhai Tong","Gaofeng Meng","Véronique Prinet","LuBin Weng"],"categories":[],"content":"","date":2021,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":2021,"objectID":"29d921d9ebe34fb820fee044e06c137b","permalink":"https://MSALab-PKU.github.io/publication/enhanced-2021/","publishdate":"1970-01-01T08:33:41+08:00","relpermalink":"/publication/enhanced-2021/","section":"publication","summary":"Glass-like objects such as windows, bottles, and mirrors exist widely in the real world. Sensing these objects has many applications, including robot navigation and grasping. However, this task is very challenging due to the arbitrary scenes behind glass-like objects. This paper aims to solve the glass-like object segmentation problem via enhanced boundary learning. In particular, we first propose a novel refined differential module that outputs finer boundary cues. We then introduce an edge-aware point-based graph convolution network module to model the global shape along the boundary. We use these two modules to design a decoder that generates accurate and clean segmentation results, especially on the object contours. Both modules are lightweight and effective: they can be embedded into various segmentation models. In extensive experiments on three recent glass-like object segmentation datasets, including Trans10k, MSD, and GDD, our approach establishes new state-of-the-art results. We also illustrate the strong generalization properties of our method on three generic segmentation datasets, including Cityscapes, BDD, and COCO Stuff. Code and models will be available for further research.","tags":[],"title":"Enhanced boundary learning for glass-like object segmentation","type":"publication"},{"authors":["Yueran Bai","Yingying Wang","Yunhai Tong","Yang Yang","Qiyue Liu","Junhui Liu"],"categories":[],"content":"","date":2020,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":2020,"objectID":"e04ebe93560c45ab300ecb086ca9e5ab","permalink":"https://MSALab-PKU.github.io/publication/boundary-2020/","publishdate":"1970-01-01T08:33:40+08:00","relpermalink":"/publication/boundary-2020/","section":"publication","summary":"Temporal action proposal generation plays an important role in video action understanding, which requires localizing high-quality action content precisely. However, generating temporal proposals with both precise boundaries and high-quality action content is extremely challenging. To address this issue, we propose a novel Boundary Content Graph Neural Network (BC-GNN) to model the insightful relations between the boundary and action content of temporal proposals by the graph neural networks. In BC-GNN, the boundaries and content of temporal proposals are taken as the nodes and edges of the graph neural network, respectively, where they are spontaneously linked. Then a novel graph computation operation is proposed to update features of edges and nodes. After that, one updated edge and two nodes it connects are used to predict boundary probabilities and content confidence score, which will be combined to generate a final high-quality proposal. Experiments are conducted on two mainstream datasets: ActivityNet-1.3 and THUMOS14. Without the bells and whistles, BC-GNN outperforms previous state-ofthe-art methods in both temporal action proposal and temporal action detection tasks.","tags":[],"title":"Boundary content graph neural network for temporal action proposal generation","type":"publication"},{"authors":["Xiangtai Li","Xia Li","Li Zhang","Guangliang Cheng","Jianping Shi","Zhouchen Lin","Shaohua Tan","Yunhai Tong"],"categories":[],"content":"","date":2020,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":2020,"objectID":"96b103c67d4ad5e4021e1120fcecac62","permalink":"https://MSALab-PKU.github.io/publication/decouplesegnets-2020/","publishdate":"1970-01-01T08:33:40+08:00","relpermalink":"/publication/decouplesegnets-2020/","section":"publication","summary":"Existing semantic segmentation approaches either aim to improve the object’s inner consistency by modeling the global context, or refine objects detail along their boundaries by multi-scale feature fusion. In this paper, a new paradigm for semantic segmentation is proposed. Our insight is that appealing performance of semantic segmentation requires explicitly modeling the object body and edge, which correspond to the high and low frequency of the image. To do so, we first warp the image feature by learning a flow field to make the object part more consistent. The resulting body feature and the residual edge feature are further optimized under decoupled supervision by explicitly sampling different parts (body or edge) pixels. We show that the proposed framework with various baselines or backbone networks leads to better object inner consistency and object boundaries. Extensive experiments on four major road scene semantic segmentation benchmarks including Cityscapes, CamVid , KIITI and BDD show that our proposed approach establishes new state of the art while retaining high efficiency in inference. In particular, we achieve 83.7 mIoU % on Cityscape with only fine-annotated data. Code and models are made available to foster any further research (https://github.com/lxtGH/DecoupleSegNets).","tags":[],"title":"Improving semantic segmentation via decoupled body and edge supervision","type":"publication"},{"authors":["Xiangtai Li","Ansheng You","Zhen Zhu","Houlong Zhao","Maoke Yang","Kuiyuan Yang","Shaohua Tan","Yunhai Tong"],"categories":[],"content":"","date":2020,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":2020,"objectID":"12fdec90910559558a69d27df741c810","permalink":"https://MSALab-PKU.github.io/publication/sfsegnets-2020/","publishdate":"1970-01-01T08:33:40+08:00","relpermalink":"/publication/sfsegnets-2020/","section":"publication","summary":"In this paper, we focus on designing effective method for fast and accurate scene parsing. A common practice to improve the performance is to attain high resolution feature maps with strong semantic representation. Two strategies are widely used—atrous convolutions and feature pyramid fusion, are either computation intensive or ineffective. Inspired by the Optical Flow for motion alignment between adjacent video frames, we propose a Flow Alignment Module (FAM) to learn Semantic Flow between feature maps of adjacent levels, and broadcast highlevel features to high resolution features effectively and efficiently. Furthermore, integrating our module to a common feature pyramid structure exhibits superior performance over other real-time methods even on lightweight backbone networks, such as ResNet-18. Extensive experiments are conducted on several challenging datasets, including Cityscapes, PASCAL Context, ADE20K and CamVid. Especially, our network is the first to achieve 80.4% mIoU on Cityscapes with a frame rate of 26 FPS. The code is available at https://github.com/lxtGH/SFSegNets.","tags":[],"title":"Semantic flow for fast and accurate scene parsing","type":"publication"},{"authors":["Defu Cao","Yujing Wang","Juanyong Duan","Ce Zhang","Xia Zhu","Congrui Huang","Yunhai Tong","Bixiong Xu","Jing Bai","Jie Tong","Qi Zhang"],"categories":[],"content":"","date":2020,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":2020,"objectID":"9cadb24c69ac00bcb8b4ec47bbcf1c5e","permalink":"https://MSALab-PKU.github.io/publication/spectral-2020/","publishdate":"1970-01-01T08:33:40+08:00","relpermalink":"/publication/spectral-2020/","section":"publication","summary":"Multivariate time-series forecasting plays a crucial role in many real-world applications. It is a challenging problem as one needs to consider both intra-series temporal correlations and inter-series correlations simultaneously. Recently, there have been multiple works trying to capture both correlations, but most, if not all of them only capture temporal correlations in the time domain and resort to pre-defined priors as inter-series relationships.","tags":[],"title":"Spectral temporal graph neural network for multivariate time-series forecasting","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://MSALab-PKU.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://MSALab-PKU.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":["Li Zhang","Xiangtai Li","Anurag Arnab","Kuiyuan Yang","Yunhai Tong","Philip HS Torr"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7709ed25effaa8d21369429c394fda17","permalink":"https://MSALab-PKU.github.io/publication/dgcnet-2019/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/dgcnet-2019/","section":"publication","summary":"Exploiting long-range contextual information is key for pixel-wise prediction tasks such as semantic segmentation. In contrast to previous work that uses multi-scale feature fusion or dilated convolutions, we propose a novel graph-convolutional network (GCN) to address this problem. Our Dual Graph Convolutional Network (DGCNet) models the global context of the input feature by modelling two orthogonal graphs in a single framework. The first component models spatial relationships between pixels in the image, whilst the second models interdependencies along the channel dimensions of the network's feature map. This is done efficiently by projecting the feature into a new, lower-dimensional space where all pairwise interactions can be modelled, before reprojecting into the original space. Our simple method provides substantial benefits over a strong baseline and achieves state-of-the-art results on both Cityscapes (82.0% mean IoU) and Pascal Context (53.7% mean IoU) datasets. Code and models are made available to foster any further research (https://github.com/lxtGH/GALD-DGCNet).","tags":[],"title":"Dual graph convolutional network for semantic segmentation","type":"publication"},{"authors":["Lu He","Qianyu Zhou","Xiangtai Li","Li Niu","Guangliang Cheng","Xiao Li","Wenxuan Liu","Yunhai Tong","Lizhuang Ma","Liqing Zhang"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8d5703285a873f9f62b618bba4f90fbc","permalink":"https://MSALab-PKU.github.io/publication/transvod-2021/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/transvod-2021/","section":"publication","summary":"Recently, DETR and Deformable DETR have been proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance as previous complex hand-crafted detectors. However, their performance on Video Object Detection (VOD) has not been well explored. In this paper, we present TransVOD, an end-to-end video object detection model based on a spatial-temporal Transformer architecture. The goal of this paper is to streamline the pipeline of VOD, effectively removing the need for many hand-crafted components for feature aggregation, e.g., optical flow, recurrent neural networks, relation networks. Besides, benefited from the object query design in DETR, our method does not need complicated post-processing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean. In particular, we present temporal Transformer to aggregate both the spatial object queries and the feature memories of each frame. Our temporal Transformer consists of three components: Temporal Deformable Transformer Encoder (TDTE) to encode the multiple frame spatial details, Temporal Query Encoder (TQE) to fuse object queries, and Temporal Deformable Transformer Decoder to obtain current frame detection results. These designs boost the strong baseline deformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID dataset. TransVOD yields comparable results performance on the benchmark of ImageNet VID. We hope our TransVOD can provide a new perspective for video object detection. Code will be made publicly available at https://github.com/SJTU-LuHe/TransVOD.","tags":[],"title":"End-to-end video object detection with spatial-temporal transformers","type":"publication"},{"authors":["Yujing Wang","Yaming Yang","Jiangang Bai","Mingliang Zhang","Jing Bai","Jing Yu","Ce Zhang","Gao Huang","Yunhai Tong"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"898fbd6b34f17833bd48730befdaa709","permalink":"https://MSALab-PKU.github.io/publication/evolving-2021/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/evolving-2021/","section":"publication","summary":"Transformer is a ubiquitous model for natural language processing and has attracted wide attentions in computer vision. The attention maps are indispensable for a transformer model to encode the dependencies among input tokens. However, they are learned independently in each layer and sometimes fail to capture precise patterns. In this paper, we propose a novel and generic mechanism based on evolving attention to improve the performance of transformers. On one hand, the attention maps in different layers share common knowledge, thus the ones in preceding layers can instruct the attention in succeeding layers through residual connections. On the other hand, low-level and high-level attentions vary in the level of abstraction, so we adopt convolutional layers to model the evolutionary process of attention maps. The proposed evolving attention mechanism achieves significant performance improvement over various state-of-the-art models for multiple tasks, including image classification, natural language understanding and machine translation.","tags":[],"title":"Evolving attention with residual convolutions","type":"publication"},{"authors":["Shilin Xu","Xiangtai Li","Jingbo Wang","Guangliang Cheng","Yunhai Tong","Dacheng Tao"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bbe6c87de6299da2b73129897434ea33","permalink":"https://MSALab-PKU.github.io/publication/fashionformer-2022/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/fashionformer-2022/","section":"publication","summary":"Human fashion understanding is one crucial computer vision task since it has comprehensive information for real-world applications. This focus on joint human fashion segmentation and attribute recognition. Contrary to the previous works that separately model each task as a multi-head prediction problem, our insight is to bridge these two tasks with one unified model via vision transformer modeling to benefit each task. In particular, we introduce the object query for segmentation and the attribute query for attribute prediction. Both queries and their corresponding features can be linked via mask prediction. Then we adopt a two-stream query learning framework to learn the decoupled query representations.We design a novel Multi-Layer Rendering module for attribute stream to explore more fine-grained features. The decoder design shares the same spirit as DETR. Thus we name the proposed method Fahsionformer. Extensive experiments on three human fashion datasets illustrate the effectiveness of our approach. In particular, our method with the same backbone achieve relative 10% improvements than previous works in case of a joint metric (APmask IoU+F 1 ) for both segmentation and attribute recognition. To the best of our knowledge, we are the first unified end-to-end vision transformer framework for human fashion analysis. We hope this simple yet effective method can serve as a new flexible baseline for fashion analysis. Code will be available https://github.com/xushilin1/FashionFormer.","tags":[],"title":"Fashionformer: A simple, effective and unified baseline for human fashion segmentation and recognition","type":"publication"},{"authors":["Xiangtai Li","Houlong Zhao","Lei Han","Yunhai Tong","Shaohua Tan","Kuiyuan Yang"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e2c26d252f52b737235b4eaf56e7ffa7","permalink":"https://MSALab-PKU.github.io/publication/gated-2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/gated-2020/","section":"publication","summary":"Semantic segmentation generates comprehensive understanding of scenes through densely predicting the category for each pixel. High-level features from Deep Convolutional Neural Networks already demonstrate their effectiveness in semantic segmentation tasks, however the coarse resolution of high-level features often leads to inferior results for small/thin objects where detailed information is important. It is natural to consider importing low level features to compensate for the lost detailed information in high-level features. Unfortunately, simply combining multi-level features suffers from the semantic gap among them. In this paper, we propose a new architecture, named Gated Fully Fusion (GFF), to selectively fuse features from multiple levels using gates in a fully connected way. Specifically, features at each level are enhanced by higher-level features with stronger semantics and lower-level features with more details, and gates are used to control the propagation of useful information which significantly reduces the noises during fusion. We achieve the state of the art results on four challenging scene parsing datasets including Cityscapes, Pascal Context, COCO-stuff and ADE20K.","tags":[],"title":"Gated fully fusion for semantic segmentation","type":"publication"},{"authors":["Xiangtai Li","Li Zhang","Guangliang Cheng","Kuiyuan Yang","Yunhai Tong","Xiatian Zhu","Tao Xiang"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"84b48beb8bd1cc9caa671abe41e10316","permalink":"https://MSALab-PKU.github.io/publication/dgcnet-2021/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/dgcnet-2021/","section":"publication","summary":"Modelling long-range contextual relationships is critical for pixel-wise prediction tasks such as semantic segmentation. However, convolutional neural networks (CNNs) are inherently limited to model such dependencies due to the naive structure in its building modules (e.g., local convolution kernel). While recent global aggregation methods are beneficial for longrange structure information modelling, they would oversmooth and bring noise to the regions contain fine details (e.g., boundaries and small objects), which are very much cared in the semantic segmentation task. To alleviate this problem, we propose to explore the local context for making the aggregated long-range relationship being distributed more accurately in local regions. In particular, we design a novel local distribution module which models the affinity map between global and local relationship for each pixel adaptively. Integrating existing global aggregation modules, we show that our approach can be modularized as an end-to-end trainable block and easily plugged into existing semantic segmentation networks, giving rise to the GALD networks. Despite its simplicity and versatility, our approach allows us to build new state of the art on major semantic segmentation benchmarks including Cityscapes, ADE20K, Pascal Context, Camvid and COCO-stuff. Code and trained models are released at https://github.com/lxtGH/GALD-DGCNet to foster further research.","tags":[],"title":"Global aggregation then local distribution for scene parsing","type":"publication"},{"authors":["Tianmeng Yang","Yujing Wang","Zhihan Yue","Yaming Yang","Yunhai Tong","Jing Bai"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"549ea3a7c30d85b0fd80aad695023483","permalink":"https://MSALab-PKU.github.io/publication/graph-2022/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/graph-2022/","section":"publication","summary":"Graph Neural Networks (GNNs) have shown advantages in various graph-based applications. Most existing GNNs assume strong homophily of graph structure and apply permutation-invariant local aggregation of neighbors to learn a representation for each node. However, they fail to generalize to heterophilic graphs, where most neighboring nodes have different labels or features, and the relevant nodes are distant. Few recent studies attempt to address this problem by combining multiple hops of hidden representations of central nodes (i.e., multi-hop-based approaches) or sorting the neighboring nodes based on attention scores (i.e., rankingbased approaches). As a result, these approaches have some apparent limitations. On the one hand, multi-hop-based approaches do not explicitly distinguish relevant nodes from a large number of multi-hop neighborhoods, leading to a severe over-smoothing problem. On the other hand, ranking-based models do not joint-optimize node ranking with end tasks and result in sub-optimal solutions. In this work, we present Graph Pointer Neural Networks (GPNN) to tackle the challenges mentioned above. We leverage a pointer network to select the most relevant nodes from a large amount of multihop neighborhoods, which constructs an ordered sequence according to the relationship with the central node. 1D convolution is then applied to extract high-level features from the node sequence. The pointer-network-based ranker in GPNN is joint-optimized with other parts in an end-to-end manner. Extensive experiments are conducted on six public node classifcation datasets with heterophilic graphs. The results show that GPNN signifcantly improves the classifcation performance of state-of-the-art methods. In addition, analyses also reveal the privilege of the proposed GPNN in fltering out irrelevant neighbors and reducing over-smoothing.","tags":[],"title":"Graph pointer neural networks","type":"publication"},{"authors":["Yiren Chen","XiaoYu Kou","Jiangang Bai","Yunhai Tong"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2b6578fc61282bf970f06f051c5feb75","permalink":"https://MSALab-PKU.github.io/publication/ssa-2021/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/ssa-2021/","section":"publication","summary":"One of the most popular paradigms of applying large pre-trained NLP models such as BERT is to fine-tune it on a smaller dataset. However, one challenge remains as the fine-tuned model often overfits on smaller datasets. A symptom of this phenomenon is that irrelevant or misleading words in the sentence, which are easy to understand for human beings, can substantially degrade the performance of these fine-tuned BERT models. In this paper, we propose a novel technique, called Self-Supervised Attention (SSA) to help facilitate this generalization challenge. Specifically, SSA automatically generates weak, token-level attention labels iteratively by probing the fine-tuned model from the previous iteration. We investigate two different ways of integrating SSA into BERT and propose a hybrid approach to combine their benefits. Empirically, through a variety of public datasets, we illustrate significant performance improvement using our SSA-enhanced BERT model.","tags":[],"title":"Improving BERT with Self-Supervised Attention","type":"publication"},{"authors":["Xiangtai Li","Hao He","Yibo Yang","Henghui Ding","Kuiyuan Yang","Guangliang Cheng","Yunhai Tong","Dacheng Tao"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d40fc09f7e98333888930fd601c16389","permalink":"https://MSALab-PKU.github.io/publication/tpr-2021/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/tpr-2021/","section":"publication","summary":"Video Instance Segmentation (VIS) is a new and inherently multi-task problem, which aims to detect, segment and track each instance in a video sequence. Existing approaches are mainly based on single-frame features or single-scale features of multiple frames, where temporal information or multi-scale information is ignored. To incorporate both temporal and scale information, we propose a Temporal Pyramid Routing (TPR) strategy to conditionally align and conduct pixel-level aggregation from a feature pyramid pair of two adjacent frames. Specifically, TPR contains two novel components, including Dynamic Aligned Cell Routing (DACR) and Cross Pyramid Routing (CPR), where DACR is designed for aligning and gating pyramid features across temporal dimension, while CPR transfers temporally aggregated features across scale dimension. Moreover, our approach is a plug-and-play module and can be easily applied to existing instance segmentation methods. Extensive experiments on YouTube-VIS dataset demonstrate the effectiveness and efficiency of the proposed approach on several state-of-the-art instance segmentation methods. Codes and trained models will be publicly available to facilitate future research.(https://github.com/lxtGH/TemporalPyramidRouting).","tags":[],"title":"Improving Video Instance Segmentation via Temporal Pyramid Routing","type":"publication"},{"authors":["Tianmeng Yang","Min Zhou","Yujing Wang","Zhengjie Lin","Lujia Pan","Bin Cui","Yunhai Tong"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0dceb85ea12a5eb514de1e833bc1e018","permalink":"https://MSALab-PKU.github.io/publication/mitigating-2023/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/mitigating-2023/","section":"publication","summary":"Graph Active Learning (GAL), which aims to find the most informative nodes in graphs for annotation to maximize the Graph Neural Networks (GNNs) performance, has attracted many research efforts but remains non-trivial challenges. One major challenge is that existing GAL strategies may introduce semantic confusion to the selected training set, particularly when graphs are noisy. Specifically, most existing methods assume all aggregating features to be helpful, ignoring the semantically negative effect between inter-class edges under the message-passing mechanism. In this work, we present Semantic-aware Active learning framework for Graphs (SAG) to mitigate the semantic confusion problem. Pairwise similarities and dissimilarities of nodes with semantic features are introduced to jointly evaluate the node influence. A new prototypebased criterion and query policy are also designed to maintain diversity and class balance of the selected nodes, respectively. Extensive experiments on the public benchmark graphs and a real-world financial dataset demonstrate that SAG significantly improves node classification performances and consistently outperforms previous methods. Moreover, comprehensive analysis and ablation study also verify the effectiveness of the proposed framework.","tags":[],"title":"Mitigating Semantic Confusion from Hostile Neighborhood for Graph Active Learning","type":"publication"},{"authors":["Hang Zhao","Yujing Wang","Juanyong Duan","Congrui Huang","Defu Cao","Yunhai Tong","Bixiong Xu","Jing Bai","Jie Tong","Qi Zhang"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"307cb7cd9433eb2cacc6e2b8c6c82794","permalink":"https://MSALab-PKU.github.io/publication/multivariate-2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/multivariate-2020/","section":"publication","summary":"Anomaly detection on multivariate time-series is of great importance in both data mining research and industrial applications. Recent approaches have achieved significant progress in this topic, but there is remaining limitations. One major limitation is that they do not capture the relationships between different time-series explicitly, resulting in inevitable false alarms. In this paper, we propose a novel self-supervised framework for multivariate time-series anomaly detection to address this issue. Our framework considers each univariate time-series as an individual feature and includes two graph attention layers in parallel to learn the complex dependencies of multivariate time-series in both temporal and feature dimensions. In addition, our approach jointly optimizes a forecasting-based model and a reconstruction-based model, obtaining better time-series representations through a combination of single-timestamp prediction and reconstruction of the entire time-series. We demonstrate the efficacy of our model through extensive experiments. The proposed method outperforms other state-of-the-art models on three real-world datasets. Further analysis shows that our method has good interpretability and is useful for anomaly diagnosis.","tags":[],"title":"Multivariate time-series anomaly detection via graph attention network","type":"publication"},{"authors":["Xiangtai Li","Shilin Xu","Yibo Yang","Guangliang Cheng","Yunhai Tong","Dacheng Tao"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"849dcf2b7b13973ab079462eb8e285d5","permalink":"https://MSALab-PKU.github.io/publication/partformer-2022/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/partformer-2022/","section":"publication","summary":"Panoptic Part Segmentation (PPS) aims to unify panoptic segmentation and part segmentation into one task. Previous work mainly utilizes separated approaches to handle thing, stuff, and part predictions individually without performing any shared computation and task association. In this work, we aim to unify these tasks at the architectural level, designing the first end-to-end unified method named PanopticPartFormer. In particular, motivated by the recent progress in Vision Transformer, we model things, stuff, and part as object queries and directly learn to optimize the all three predictions as unified mask prediction and classification problem. We design a decoupled decoder to generate part feature and thing/stuff feature respectively. Then we propose to utilize all the queries and corresponding features to perform reasoning jointly and iteratively. The final mask can be obtained via inner product between queries and the corresponding features. The extensive ablation studies and analysis prove the effectiveness of our framework. Our Panoptic-PartFormer achieves the new state-of-the-art results on both Cityscapes PPS and Pascal Context PPS datasets with around 70% GFlops and 50% parameters decrease. Given its effectiveness and conceptual simplicity, we hope the Panoptic-PartFormer can serve as a strong baseline and aid future research in PPS. Our code and models will be available at https://github.com/lxtGH/Panoptic-PartFormer.","tags":[],"title":"Panoptic-partformer: Learning a unified model for panoptic part segmentation","type":"publication"},{"authors":["Xiangtai Li","Hao He","Xia Li","Duo Li","Guangliang Cheng","Jianping Shi","Lubin Weng","Yunhai Tong","Zhouchen Lin"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a4738e8454d220bc03a1dbd6d6148d0c","permalink":"https://MSALab-PKU.github.io/publication/pointflow-2021/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/pointflow-2021/","section":"publication","summary":"Aerial Image Segmentation is a particular semantic segmentation problem and has several challenging characteristics that general semantic segmentation does not have. There are two critical issues: The one is an extremely foreground-background imbalanced distribution, and the other is multiple small objects along with the complex background. Such problems make the recent dense affinity context modeling perform poorly even compared with baselines due to over-introduced background context. To handle these problems, we propose a point-wise affinity propagation module based on the Feature Pyramid Network (FPN) framework, named PointFlow. Rather than dense affinity learning, a sparse affinity map is generated upon selected points between the adjacent features, which reduces the noise introduced by the background while keeping efficiency. In particular, we design a dual point matcher to select points from the salient area and object boundaries, respectively. Experimental results on three different aerial segmentation datasets suggest that the proposed method is more effective and efficient than state-of-the-art general semantic segmentation methods. Especially, our methods achieve the best speed and accuracy trade-off on three aerial benchmarks. Further experiments on three general semantic segmentation datasets prove the generality of our method. Code and models are made available (https://github.com/lxtGH/PFSegNets).","tags":[],"title":"PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation","type":"publication"},{"authors":["Haobo Yuan","Xiangtai Li","Yibo Yang","Guangliang Cheng","Jing Zhang","Yunhai Tong","Lefei Zhang","Dacheng Tao"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"06d01efe12ec6aaedb1810f7c31607ca","permalink":"https://MSALab-PKU.github.io/publication/polyphonicofrmer-2022/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/polyphonicofrmer-2022/","section":"publication","summary":"The Depth-aware Video Panoptic Segmentation (DVPS) is a new challenging vision problem that aims to predict panoptic segmentation and depth in a video simultaneously. The previous work solves this task by extending the existing panoptic segmentation method with an extra dense depth prediction and instance tracking head. However, the relationship between the depth and panoptic segmentation is not well explored – simply combining existing methods leads to competition and needs carefully weight balancing. In this paper, we present PolyphonicFormer, a vision transformer to unify these sub-tasks under the DVPS task and lead to more robust results. Our principal insight is that the depth can be harmonized with the panoptic segmentation with our proposed new paradigm of predicting instance level depth maps with object queries. Then the relationship between the two tasks via query-based learning is explored. From the experiments, we demonstrate the benefits of our design from both depth estimation and panoptic segmentation aspects. Since each thing query also encodes the instance-wise information, it is natural to perform tracking directly with appearance learning. Our method achieves state-of-the-art results on two DVPS datasets (Semantic KITTI, Cityscapes), and ranks 1st on the ICCV-2021 BMTT Challenge video + depth track.","tags":[],"title":"Polyphonicformer: Unified query learning for depth-aware video panoptic segmentation","type":"publication"},{"authors":["Yujing Wang","Yunhai Tong","Ming Zeng"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e56ca65fdbcc6d77e53c532e167605ad","permalink":"https://MSALab-PKU.github.io/publication/ranking-2013/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/ranking-2013/","section":"publication","summary":"Ranking scientific articles is an important but challenging task, partly due to the dynamic nature of the evolving publication network. In this paper, we mainly focus on two problems:(1) how to rank articles in the heterogeneous network; and (2) how to use time information in the dynamic network in order to obtain a better ranking result. To tackle the problems, we propose a graph based ranking method, which utilizes citations, authors, journals/conferences and the publication time information collaboratively. The experiments were carried out on two public datasets. The result shows that our approach is practical and ranks scientific articles more accurately than the state-of-art methods.","tags":[],"title":"Ranking scientific articles by exploiting citations, authors, journals, and time information","type":"publication"},{"authors":["Xiangtai Li","Jiangning Zhang","Yibo Yang","Guangliang Cheng","Kuiyuan Yang","Yunhai Tong","Dacheng Tao"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e7958d15f881013b3512b6f2e2af9026","permalink":"https://MSALab-PKU.github.io/publication/sfnet-2024/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/sfnet-2024/","section":"publication","summary":"In this paper, we focus on exploring effective methods for faster and accurate semantic segmentation. A common practice to improve the performance is to attain high-resolution feature maps with strong semantic representation. Two strategies are widely used: atrous convolutions and feature pyramid fusion, while both are either computationally intensive or ineffective. Inspired by the Optical Flow for motion alignment between adjacent video frames, we propose a Flow Alignment Module (FAM) to learn Semantic Flow between feature maps of adjacent levels and broadcast high-level features to high-resolution features effectively and efficiently. Furthermore, integrating our FAM to a standard feature pyramid structure exhibits superior performance over other real-time methods, even on lightweight backbone networks, such as ResNet-18 and DFNet. Then to further speed up the inference procedure, we also present a novel Gated Dual Flow Alignment Module to directly align high-resolution feature maps and low-resolution feature maps where we term the improved version network as SFNet-Lite. Extensive experiments are conducted on several challenging datasets, where results show the effectiveness of both SFNet and SFNet-Lite. In particular, when using Cityscapes test set, the SFNet-Lite series achieve 80.1 mIoU while running at 60 FPS using ResNet-18 backbone and 78.8 mIoU while running at 120 FPS using STDC backbone on RTX-3090. Moreover, we unify four challenging driving datasets (i.e., Cityscapes, Mapillary, IDD, and BDD) into one large dataset, which we named Unified Driving Segmentation (UDS) dataset. It contains diverse domain and style information. We benchmark several representative works on UDS. Both SFNet and SFNet-Lite still achieve the best speed and accuracy trade-off on UDS, which serves as a strong baseline in such a challenging setting. The code and models are publicly available at https://github.com/lxtGH/SFSegNets.","tags":[],"title":"Sfnet: Faster and accurate semantic segmentation via semantic flow","type":"publication"},{"authors":["Pengyu Zhao","Ansheng You","Yuanxing Zhang","Jiaying Liu","Kaigui Bian","Yunhai Tong"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0ea7cbccf8037a981afdea8cf672dc20","permalink":"https://MSALab-PKU.github.io/publication/spherical-2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/spherical-2020/","section":"publication","summary":"With the advance of omnidirectional panoramic technology, 360◦ imagery has become increasingly popular in the past few years. To better understand the 360◦ content, many works resort to the 360◦ object detection and various criteria have been proposed to bound the objects and compute the intersection-over-union (IoU) between bounding boxes based on the common equirectangular projection (ERP) or perspective projection (PSP). However, the existing 360◦ criteria are either inaccurate or inefficient for real-world scenarios. In this paper, we introduce a novel spherical criteria for fast and accurate 360◦ object detection, including both spherical bounding boxes and spherical IoU (SphIoU). Based on the spherical criteria, we propose a novel two-stage 360◦ detector, ie, Reprojection R-CNN, by combining the advantages of both ERP and PSP, yielding efficient and accurate 360◦ object detection. To validate the design of spherical criteria and Reprojection R-CNN, we construct two unbiased synthetic datasets for training and evaluation. Experimental results reveal that compared with the existing criteria, the two-stage detector with spherical criteria achieves the best mAP results under the same inference speed, demonstrating that the spherical criteria can be more suitable for 360◦ object detection. Moreover, Reprojection R-CNN outperforms the previous state-of-the-art methods by over 30% on mAP with competitive speed, which confirms the efficiency and accuracy of the design.","tags":[],"title":"Spherical criteria for fast and accurate 360 object detection","type":"publication"},{"authors":["Jiangang Bai","Yujing Wang","Yiren Chen","Yaming Yang","Jing Bai","Jing Yu","Yunhai Tong"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e485792bbe91daf6e1e1bc6a8b47d9de","permalink":"https://MSALab-PKU.github.io/publication/syntax-bert-2021/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/syntax-bert-2021/","section":"publication","summary":"Pre-trained language models like BERT achieve superior performances in various NLP tasks without explicit consideration of syntactic information. Meanwhile, syntactic information has been proved to be crucial for the success of NLP applications. However, how to incorporate the syntax trees effectively and efficiently into pre-trained Transformers is still unsettled. In this paper, we address this problem by proposing a novel framework named Syntax-BERT. This framework works in a plug-and-play mode and is applicable to an arbitrary pre-trained checkpoint based on Transformer architecture. Experiments on various datasets of natural language understanding verify the effectiveness of syntax trees and achieve consistent improvement over multiple pre-trained models, including BERT, RoBERTa, and T5. At the same time, we also made our experiment code public.","tags":[],"title":"Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees","type":"publication"},{"authors":["Yujing Wang","Yaming Yang","Yiren Chen","Jing Bai","Ce Zhang","Guinan Su","Xiaoyu Kou","Yunhai Tong","Mao Yang","Lidong Zhou"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dbbff262a1335241c5ab9b500c81f689","permalink":"https://MSALab-PKU.github.io/publication/textnas-2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/textnas-2020/","section":"publication","summary":"Learning text representation is crucial for text classification and other language related tasks. There are a diverse set of text representation networks in the literature, and how to find the optimal one is a non-trivial problem. Recently, the emerging Neural Architecture Search (NAS) techniques have demonstrated good potential to solve the problem. Nevertheless, most of the existing works of NAS focus on the search algorithms and pay little attention to the search space. In this paper, we argue that the search space is also an important human prior to the success of NAS in different applications. Thus, we propose a novel search space tailored for text representation. Through automatic search, the discovered network architecture outperforms state-of-the-art models on various public datasets on text classification and natural language inference tasks. Furthermore, some of the design principles found in the automatic network agree well with human intuition.","tags":[],"title":"Textnas: A neural architecture search space tailored for text representation","type":"publication"},{"authors":["Xiangtai Li","Xia Li","Ansheng You","Li Zhang","Guangliang Cheng","Kuiyuan Yang","Yunhai Tong","Zhouchen Lin"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"475b76c28dc0ac0049ac663271fd0a23","permalink":"https://MSALab-PKU.github.io/publication/sfsegnets-2021/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/sfsegnets-2021/","section":"publication","summary":"Graph-based convolutional model such as non-local block has shown to be effective for strengthening the context modeling ability in convolutional neural networks (CNNs). However, its pixel-wise computational overhead is prohibitive which renders it unsuitable for high resolution imagery. In this paper, we explore the efficiency of context graph reasoning and propose a novel framework called Squeeze Reasoning. Instead of propagating information on the spatial map, we first learn to squeeze the input feature into a channel-wise global vector and perform reasoning within the single vector where the computation cost can be significantly reduced. Specifically, we build the node graph in the vector where each node represents an abstract semantic concept. The refined feature within the same semantic category results to be consistent, which is thus beneficial for downstream tasks. We show that our approach can be modularized as an end-to-end trained block and can be easily plugged into existing networks. Despite its simplicity and being lightweight, the proposed strategy allows us to establish the considerable results on different semantic segmentation datasets and shows significant improvements with respect to strong baselines on various other scene understanding tasks including object detection, instance segmentation and panoptic segmentation. Code is available at https://github.com/lxtGH/SFSegNets.","tags":[],"title":"Towards efficient scene understanding via squeeze reasoning","type":"publication"},{"authors":["Jianzong Wu","Xiangtai Li","Shilin Xu","Haobo Yuan","Henghui Ding","Yibo Yang","Xia Li","Jiangning Zhang","Yunhai Tong","Xudong Jiang","Bernard Ghanem","Dacheng Tao"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ea0bf70f3f2fe2274e75c74f7c75aef9","permalink":"https://MSALab-PKU.github.io/publication/towards-2024/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/towards-2024/","section":"publication","summary":"In the field of visual scene understanding, deep neural networks have made impressive advancements in various core tasks like segmentation, tracking, and detection. However, most approaches operate on the close-set assumption, meaning that the model can only identify pre-defined categories that are present in the training set. Recently, open vocabulary settings were proposed due to the rapid progress of vision language pre-training. These new approaches seek to locate and recognize categories beyond the annotated label space. The open vocabulary approach is more general, practical, and effective than weakly supervised and zero-shot settings. This paper thoroughly reviews open vocabulary learning, summarizing and analyzing recent developments in the field. In particular, we begin by juxtaposing open vocabulary learning with analogous concepts such as zero-shot learning, open-set recognition, and out-of-distribution detection. Subsequently, we examine several pertinent tasks within the realms of segmentation and detection, encompassing long-tail problems, few-shot, and zero-shot settings. As a foundation for our method survey, we first elucidate the fundamental principles of detection and segmentation in close-set scenarios. Next, we examine various contexts where open vocabulary learning is employed, pinpointing recurring design elements and central themes. This is followed by a comparative analysis of recent detection and segmentation methodologies in commonly used datasets and benchmarks. Our review culminates with a synthesis of insights, challenges, and discourse on prospective research trajectories. To our knowledge, this constitutes the inaugural exhaustive literature review on open vocabulary learning. We keep tracing related works at https://github.com/jianzongwu/Awesome-Open-Vocabulary.","tags":[],"title":"Towards open vocabulary learning: A survey","type":"publication"},{"authors":["Jianzong Wu","Xiangtai Li","Xia Li","Henghui Ding","Yunhai Tong","Dacheng Tao"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a7de010246a793d690d2d89a655aab03","permalink":"https://MSALab-PKU.github.io/publication/robust-2024/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/robust-2024/","section":"publication","summary":"Referring Image Segmentation (RIS) is a fundamental vision-language task that outputs object masks based on text descriptions. Many works have achieved considerable progress for RIS, including different fusion method designs. In this work, we explore an essential question, “What if the text description is wrong or misleading?” For example, the described objects are not in the image. We term such a sentence as a negative sentence. However, existing solutions for RIS cannot handle such a setting. To this end, we propose a new formulation of RIS, named Robust Referring Image Segmentation (R-RIS). It considers the negative sentence inputs besides the regular positive text inputs. To facilitate this new task, we create three R-RIS datasets by augmenting existing RIS datasets with negative sentences and propose new metrics to evaluate both types of inputs in a unified manner. Furthermore, we propose a new transformer-based model, called RefSegformer, with a token-based vision and language fusion module. Our design can be easily extended to our R-RIS setting by adding extra blank tokens. Our proposed RefSegformer achieves state-of-the-art results on both RIS and R-RIS datasets, establishing a solid baseline for both settings.","tags":[],"title":"Towards robust referring image segmentation","type":"publication"},{"authors":["Qianyu Zhou","Xiangtai Li","Lu He","Yibo Yang","Guangliang Cheng","Yunhai Tong","Lizhuang Ma","Dacheng Tao"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"717515f12452987ad1bc3aab8aae8291","permalink":"https://MSALab-PKU.github.io/publication/transvod-v2-2021/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/transvod-v2-2021/","section":"publication","summary":"Detection Transformer (DETR) and Deformable DETR have been proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance as previous complex hand-crafted detectors. However, their performance on Video Object Detection (VOD) has *not* been well explored. In this paper, we present **TransVOD**, the first end-to-end video object detection system based on spatial-temporal Transformer architectures. The first goal of this paper is to streamline the pipeline of VOD, effectively removing the need for many hand-crafted components for feature aggregation, *e.g.*, optical flow model, relation networks. Besides, benefited from the object query design in DETR, our method does not need complicated post-processing methods such as Seq-NMS. In particular, we present a temporal Transformer to aggregate both the spatial object queries and the feature memories of each frame. Our temporal transformer consists of two components: Temporal Query Encoder (TQE) to fuse object queries, and Temporal Deformable Transformer Decoder (TDTD) to obtain current frame detection results. These designs boost the strong baseline deformable DETR by a significant margin (3 %-4 % mAP) on the ImageNet VID dataset. TransVOD yields comparable performances on the benchmark of ImageNet VID. Then, we present two improved versions of TransVOD including TransVOD++ and TransVOD Lite. The former fuses object-level information into object query via dynamic convolution while the latter models the entire video clips as the output to speed up the inference time. We give detailed analysis of all three models in the experiment part. In particular, our proposed TransVOD++ sets a new state-of-the-art record in terms of accuracy on ImageNet VID with 90.0 % mAP. Our proposed TransVOD Lite also achieves the best speed and accuracy trade-off with 83.7 % mAP while running at around 30 FPS on a single V100 GPU device. Code and models will be available for further research.","tags":[],"title":"TransVOD: End-to-end Video Object Detection with Spatial-Temporal Transformers","type":"publication"},{"authors":["Zhihan Yue","Yujing Wang","Juanyong Duan","Tianmeng Yang","Congrui Huang","Yunhai Tong","Bixiong Xu"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1234ccbfd962881189b813f23b17b2b6","permalink":"https://MSALab-PKU.github.io/publication/ts2vec-2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/ts2vec-2020/","section":"publication","summary":"This paper presents TS2Vec, a universal framework for learning representations of time series in an arbitrary semantic level. Unlike existing methods, TS2Vec performs contrastive learning in a hierarchical way over augmented context views, which enables a robust contextual representation for each timestamp. Furthermore, to obtain the representation of an arbitrary sub-sequence in the time series, we can apply a simple aggregation over the representations of corresponding timestamps. We conduct extensive experiments on time series classification tasks to evaluate the quality of time series representations. As a result, TS2Vec achieves significant improvement over existing SOTAs of unsupervised time series representation on 125 UCR datasets and 29 UEA datasets. The learned timestamp-level representations also achieve superior results in time series forecasting and anomaly detection tasks. A linear regression trained on top of the learned representations outperforms previous SOTAs of time series forecasting. Furthermore, we present a simple way to apply the learned representations for unsupervised anomaly detection, which establishes SOTA results in the literature. The source code is publicly available at https://github.com/yuezhihan/ts2vec.","tags":[],"title":"Ts2vec: Towards universal representation of time series","type":"publication"}]